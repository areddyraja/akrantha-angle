 <!-- Page Content -->
<div class="container" id ="doclayout" style="margin-top:30px;">
    <div class="row">

        <div class="col-md-2">
            <aside class="sidebar">

                <ul class="nav">
                    <li class="heading">Hadoop</li>
                    <li><a is-active-nav href="#introHadoop">Introduction</a></li>
                </ul>

                <ul class="nav">
                    <li class="heading">Installation</li>
                    <li><a is-active-nav href="#single-nodeInstallation">Single-node Installation</a></li>
                    <li><a is-active-nav href="#multi-nodeInstallation">Multi-node Installation</a></li>
                </ul>

                <ul class="nav">
                    <li class="heading">HDFS</li>
                    <li><a href="#hdfsIntro">Introduction</a></li>
                    <li><a href="#hdfsCommands">Try HDFS Commands</a></li>
                    <li><a href="#javaApi">Java-API</a></li>
                </ul>

                <ul class="nav">
                    <li class="heading">Hive</li>
                    <li><a href="#hiveIntro">Hive – Introduction</a></li>
                    <li><a href="#hiveRunMode">Hive – Run Modes</a></li>
                    <li><a href="#hiveBasic">Hive Basics</a></li>
                    <li><a href="#hiveDataDefination">HiveQL-Data Definition</a></li>
                    <li><a href="#hiveQueriesPart1">Hive Queries – Part 1</a></li>
                    <li><a href="#hiveQueriesPart2">Hive Queries – Part 2</a></li>
                </ul>


            </aside>
        </div>


        <div class="content">
            <div class="col-md-7" style="margin-top:-18px;">

            <div id="introHadoop" style="display:block;margin-top: -50px;"><br><br>
                <h1>Hadoop</h1>
                <hr />
                <p><strong>Hadoop :Meeting  Big data Challenge</strong></p>
                <p>Apache Hadoop meets the challenges of Big Data by simplifying the implementation of<strong> data-intensive, highly parallel distributed</strong>  applications.</p>
                <p>Hadoop is different from previous distributed approaches in the following ways:</p>
                <ul>
                    <li>Data is distributed in advance.</li>
                    <li>Data is replicated throughout a cluster of computers for reliability and availability.</li>
                    <li>Data processing tries to occur where the data is stored, thus eliminating bandwidth bottlenecks.</li>
                </ul>

                <p>Hadoop provides a powerful mechanism for data analytics, which consists of the following:</p>
                <ul>
                    <li>Vast amount of storage</li>
                    <li>Distributed processing with fast data access</li>
                    <li>Reliability, failover, and scalability</li>
                </ul>

                <p><strong>Data Science in the Business World</strong></p>
                <p>The capability of Hadoop to store and process huge amounts of data is frequently associated with “data science” .Although the term was introduced by Peter Naur in the 1960s, it did not get wide acceptance until recently.</p>
                <p>Business analysts study patterns in existing business operations to improve them.
The goal of data science is to extract meaning from data. The work of data scientists is based on math, statistical analysis, pattern recognition, machine learning,high-performance computing,data warehousing, and much more. They analyze information to look for trends, statistics, and new business possibilities based on collected information.</p>
                <p>Businesses are using Hadoop for solving business problems, with a few notable examples:</p>

                    <p>
                        <strong> Enhancing fraud detection for banks and credit card companies</strong> — Companies are utilizing Hadoop to detect transaction fraud. By providing analytics on large clusters of commodity hardware, banks are using Hadoop, applying analytic models to a full set of transactions for their clients, and providing near-real-time fraud-in-progress detection.
                    </p>
                    <p><strong> Social media marketing analysis</strong> — Companies are currently using Hadoop for brand
management, marketing campaigns, and brand protection. By monitoring, collecting, and aggregating data from various Internet sources such as blogs, boards, news feeds, tweets, and social media, companies are using Hadoop to extract and aggregate information about their products, services, and competitors, discovering patterns and revealing upcoming trends important for understanding their business.</p> 
                    <p><strong>Shopping pattern analysis for retail product placement</strong> — Businesses in the retail industry are using Hadoop to determine products most appropriate to sell in a particular store based on the store’s location and the shopping patterns of the population around it.</p>
                    <p><strong> Traffic pattern recognition for urban development</strong> — Urban development often relies
on traffic patterns to determine requirements for road network expansion. By monitoring
traffic during different times of the day and discovering patterns, urban developers can
determine traffic bottlenecks, which allow them to decide whether additional streets/street lanes are required to avoid traffic congestions during peak hours.</p>
                    <p><strong>Content optimization and engagement</strong> — Companies are focusing on optimizing content for rendering on different devices supporting different content formats. Many media companies require that a large amount of content be processed in different formats. Also, content engagement models must be mapped for feedback and enhancements.</p>
                    <p><strong>Network analytics and mediation</strong> — Real-time analytics on a large amount of data
generated in the form of usage transaction data, network performance data, cell-site
information, device-level data, and other forms of back office data is allowing companies to reduce operational expenses, and enhance the user experience on networks.</p>
                    <p><strong>Large data transformation</strong> — The New York Times needed to generate PDF files for 11
million articles (every article from 1851 to 1980) in the form of images scanned from the
original paper. Using Hadoop, the newspaper was able to convert 4 TB of scanned articles
to 1.5 TB of PDF documents in 24 hours.</p><br>
                    <p><strong>THE HADOOP ECOSYSTEM</strong></p>
                    <p>Hadoop should be classified as an ecosystem comprised of many components that range from data storage, to data integration, to data processing, to specialized tools for data analysts.</p>
                    <img src="c:/hhdsdd.png"><br>
                    <p>Hadoop’s ecosystem consists of the following:</p>
                    <p><strong>HDFS</strong> — A foundational component of the Hadoop ecosystem is the Hadoop Distributed
File System (HDFS). HDFS is the mechanism by which a large amount of data can be
distributed over a cluster of computers, and data is written once, but read many times for
analytics. It provides the foundation for other tools, such as HBase.</p>
                    <p><strong>MapReduce </strong> — Hadoop’s main execution framework is MapReduce, a programming model  for distributed, parallel data processing, breaking jobs into mapping phases and reduce phases (thus the name). Developers write MapReduce jobs for Hadoop, using data stored in HDFS for fast data access. Because of the nature of how MapReduce works, Hadoop brings the processing to the data in a parallel fashion, resulting in fast implementation.</p>
                    <p><strong>HBase</strong> — A column-oriented NoSQL database built on top of HDFS, HBase is used for fast read/write access to large amounts of data. HBase uses Zookeeper for its management to ensure that all of its components are up and running.</p>
                    <p><strong>Zookeeper</strong> — Zookeeper is Hadoop’s distributed coordination service. Designed to run over a cluster of machines, it is a highly available service used for the management of Hadoop operations, and many components of Hadoop depend on it.</p>
                    <p><strong>Oozie</strong> — A scalable workflow system, Oozie is integrated into the Hadoop stack, and is
used to coordinate execution of multiple MapReduce jobs. It is capable of managing a
significant amount of complexity, basing execution on external events that include timing
and presence of required data.</p>
                    <p><strong>Pig</strong> — An abstraction over the complexity of MapReduce programming, the Pig platform
includes an execution environment and a scripting language (Pig Latin) used to analyze
Hadoop data sets. Its compiler translates Pig Latin into sequences of MapReduce programs.</p>
                    <p><strong>Hive</strong> — An SQL-like, high-level language used to run queries on data stored in Hadoop, Hive enables developers not familiar with MapReduce to write data queries that are translated into MapReduce jobs in Hadoop. Like Pig, Hive was developed as an abstraction layer, but geared more toward database analysts more familiar with SQL than Java programming.</p><br>
                    <p>The Hadoop ecosystem also contains several frameworks for integration with the rest of the enterprise:</p>
                    <p><strong>Sqoop</strong> — is a connectivity tool for moving data between relational databases and data
warehouses and Hadoop. Sqoop leverages database to describe the schema for the imported/exported data and MapReduce for parallelization operation and fault tolerance.</p>
                    <p><strong>Flume</strong> — is a distributed, reliable, and highly available service for efficiently collecting,
aggregating, and moving large amounts of data from individual machines to HDFS. It
is based on a simple and flexible architecture, and provides a streaming of data flows. It
leverages a simple extensible data model, allowing you to move data from multiple machines within an enterprise into Hadoop.</p>
                    <p>Hadoop’s ecosystem is growing to provide newer capabilities and components, such as the following:</p>
                    <p><strong>Whirr</strong> — This is a set of libraries that allows users to easily spin-up Hadoop clusters on top
of Amazon EC2, Rackspace, or any virtual infrastructure.</p>
                    <p><strong>Mahout</strong> — This is a machine-learning and data-mining library that provides MapReduce
implementations for popular algorithms used for clustering, regression testing, and
statistical modeling.</p>
                    <p><strong>BigTop</strong> — This is a formal process and framework for packaging and interoperability
testing of Hadoop’s sub-projects and related components.</p>
                    <p><strong>Ambari</strong> — This is a project aimed at simplifying Hadoop management by providing support for provisioning, managing, and monitoring Hadoop clusters.</p>
                <hr />
            </div>


            <div id="single-nodeInstallation" style="display:block;margin-top: -50px;"><br><br>
                <h1>Single-node installation</h1>
                <hr />
                <p>The goal of the document is to get Hadoop version 2.0.3-alpha up and running on a single node cluster.</p>

                <h2>Introduction</h2> 
                <p>Hadoop is a framework written in Java for running applications on large clusters of commodity hardware and incorporates features similar to those of the Google File System (GFS) and of the MapReduce computing paradigm.</p>
                <p>The tutorial has been tested with the following software versions:</p>
                <ol>
                    <li>Ubuntu Linux 12.04 (You can download Ubuntu 12.04 from <a href="http://releases.ubuntu.com/12.04/">here</a>)</li>
                    <li>JDK 1.7 (Will also work with JDK 1.6)</li>
                </ol>

                <p>Apache Hadoop has undergone a complete overhaul with vrsion 2, (MRv2) also called as YARN(Yet Another Negotiator) YARN architecture splits the function of JobTracker into Resource Manager and per Application Master. NodeManager is similar to the Task tracker, with the task being abstracted as container. Hadoop 2.0 also comes with HDFS federation and High Avialble for namenode.</p>
                <p>The tutorial has been tested with the following software versions:</p>
                <ol>
                    <li>Ubuntu Linux 12.04 (<a href="http://releases.ubuntu.com/12.04/">download Ubuntu</a>)</li>
                    <li>JDK 1.7 ( <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html">download JDK</a>). The same tutorial works on JDK1.6 as well.</li>
                </ol>
                <h2>Prerequisites</h2>
                <p>A system with 2 GB RAM and Ubuntu 12.04 up and running.</p>
                <hr />


                <h2>Installation</h2>
                <p><strong>Create Dedicated User</strong></p>
                <p>Create a user ‘gpuser’ and group ‘gp’. We will be running Hadoop with this userid</p>

                <div class="codebox">
                    <pre>
$sudo addgroup gp
$sudo adduser --ingroup gp gpuser
                    </pre>
                </div>
                <p><strong>Install JDK</strong></p>
                <p>Ignore this step if JDK is already installed. If not, <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html">Download JDK </a>and install.</p>
                <p>Login with gpuser to proceed with further installation steps.</p>
                <p>Set JAVA_HOME by adding the following lines at the end of <strong>.bashrc</strong> file located in gpuser home directory /home/gpuser</p>
                <div class="codebox">
                    <pre>
export JAVA_HOME=$HOME/java/jdk1.7.0_17
export PATH=$PATH:$JAVA_HOME/bin
                    </pre>
                </div>
                <p>Verify the Java installation with the following command:</p>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~$source ~/.bashrc
gpuser@ubuntu:~$java -version
                    </pre>
                </div>
                <p>You should see java version 1.7.* printed on the console.</p>
                <p>Download and Extract Hadoop</p>
                <p>Download <a href="http://apache.techartifact.com/mirror/hadoop/common/hadoop-2.0.3-alpha/">Hadoop</a> version 2.0.3</p>
                <p>Extract hadoop at the home directory using the following command.</p>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~$ tar -xvzf hadoop-2.0.3-alpha.tar.gz
                    </pre>
                </div>

                <p><strong>Setup SSH</strong></p>
                <p>Hadoop requires SSH access and manage its nodes running hadoop. We will configure SSH to connect to localhost without a password</p>
                <p><strong>Install SSH</strong></p>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~$sudo apt-get install ssh
gpuser@ubuntu:~$sudo apt-get install rsync
                    </pre>
                </div>

                <p><strong>Generate public/private RSA key pairs using the following command.</strong></p>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~$ ssh-keygen -t rsa -P ""
Generating public/private rsa key pair.
Enter file in which to save the key (/home/gpuser/.ssh/id_rsa):
Created directory '/home/gpuser/.ssh'.
Your identification has been saved in /home/gpuser/.ssh/id_rsa.
Your public key has been saved in /home/gpuser/.ssh/id_rsa.pub.
The key fingerprint is:
3c:f7:ca:f2:49:a7:05:ee:90:66:05:0f:7b:a9:63:fc gpuser@ubuntu
The key's randomart image is:
+--[ RSA 2048]----+
|                 |
|                 |
|        o        |
|       . = .     |
|        S B      |
|       . O o     |
|        X o +    |
|       +.B *     |
|         oE      |
+-----------------+
                    </pre>
                </div>

                <p>Press Enter to save the key in the default location.</p>
                <p>Enable ssh to the local machine using the following command.</p>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~$ cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
                    </pre>
                </div>

                <ul>
                    <li>Finally test the ssh setup by connecting to the local machine with the gpuser user.</li>
                </ul>

                <div class="codebox">
                    <pre>
gpuser@ubuntu:~$ ssh localhost
 
The authenticity of host 'localhost (127.0.0.1)' can't be established.
ECDSA key fingerprint is ca:96:1c:5a:38:f8:9f:99:45:d4:57:82:3c:1b:64:b7.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
 
The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.
 
Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.
 
Welcome to Ubuntu 12.10 (GNU/Linux 3.5.0-17-generic i686)
 
 * Documentation:  https://help.ubuntu.com/
                    </pre>
                </div>

                <p><strong>Configure Hadoop</strong></p>
                <p>Hadoop requires the following environment variables to be set correctly</p>
                <ul>
                    <li>
                        HADOOP environement variables
                    </li>
                </ul>
                <p>Open .bashrc file in the home folder and add the following lines at the end</p>

                <div class="codebox">
                    <pre>
export HADOOP_HOME=$HOME/hadoop-2.0.3-alpha
export HADOOP_MAPRED_HOME=$HOME/hadoop-2.0.3-alpha
export HADOOP_COMMON_HOME=$HOME/hadoop-2.0.3-alpha
export HADOOP_HDFS_HOME=$HOME/hadoop-2.0.3-alpha
export YARN_HOME=$HOME/hadoop-2.0.3-alpha
export HADOOP_CONF_DIR=$HOME/hadoop-2.0.3-alpha/etc/hadoop
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin
                    </pre>
                </div>

                <ul>
                    <li>
                        Source the variables using the following command
                    </li>
                </ul>

                <div class="codebox">
                    <pre>
gpuser@ubuntu$source ~/.bashrc
                    </pre>
                </div>

                <ul>
                    <li>Update Configuration Files</li>
                </ul>
                <p>Hadoop configration files at located at $HADOOP_HOME/etc/hadoop. Update the configuration files with the following entries respectively.</p>
                 <p><strong>yarn-site.xml</strong></p>
                <div class="codebox">
               
 <pre>
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce.shuffle&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</pre>
                </div>

                <p><b>Note:</b>In case of hadoop 2.2.x installation replace <b>mapreduce.shuffle</b> with <b>mapreduce_shuffle</b></p>
                <p><strong>core-site.xml</strong></p>
<div class="codebox">
<pre>
&lt;configuration&gt;
    &lt;property&gt;
    &lt;name&gt;fs.default.name&lt;/name&gt;
    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</pre>
</div>
<p><strong>hdfs-site.xml</strong></p>
<div class="codebox">
<pre>
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
        &lt;value&gt;file:/home/gpuser/data/namenode&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
       &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
       &lt;value&gt;file:/home/gpuser/data/datanode&lt;/value&gt;
   &lt;/property&gt;
&lt;/configuration&gt;
</pre>
</div>

<p>If this file does not exist, create it and paste the content provided below:</p>
<p><strong>mapred-site.xml</strong></p>
<div class="codebox">
<pre>
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</pre>
</div>

                <hr />

                <h2>Run Hadoop</h2>
                <p><strong>Initialize the Hadoop File system</strong></p>
                <p>Namenode needs to be initialized before starting with Hadoop File System. Once intialized, Namenode creates a unique namespace id for this instance of the Hadoop File System.</p>
                <ul>
                    <li>Create the <em>Storage Directories</em> for HDFS</li>
                </ul>
                <p>Storage Directories for HDFS</p>
                <div class="codebox">
                    <pre>
$mkdir -p $HOME/data/namenode
$mkdir -p $HOME/data/datanode
                    </pre>
                </div>

                <p>Initialize the filesystem</p>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~/hadoop-2.0.3-alpha$ bin/hadoop namenode -format
 
o/p:
 
Formatting using clusterid: CID-8b844021-d1ea-4b0a-a625-d17dcc133299
13/04/04 02:07:39 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
13/04/04 02:07:39 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
13/04/04 02:07:39 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
13/04/04 02:07:39 INFO blockmanagement.BlockManager: defaultReplication         = 1
13/04/04 02:07:39 INFO blockmanagement.BlockManager: maxReplication             = 512
13/04/04 02:07:39 INFO blockmanagement.BlockManager: minReplication             = 1
13/04/04 02:07:39 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
13/04/04 02:07:39 INFO blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
13/04/04 02:07:39 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
13/04/04 02:07:39 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
13/04/04 02:07:39 INFO namenode.FSNamesystem: fsOwner             = gpuser (auth:SIMPLE)
13/04/04 02:07:39 INFO namenode.FSNamesystem: supergroup          = supergroup
13/04/04 02:07:39 INFO namenode.FSNamesystem: isPermissionEnabled = true
13/04/04 02:07:39 INFO namenode.FSNamesystem: HA Enabled: false
13/04/04 02:07:39 INFO namenode.FSNamesystem: Append Enabled: true
13/04/04 02:07:40 INFO namenode.NameNode: Caching file names occuring more than 10 times
13/04/04 02:07:40 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
13/04/04 02:07:40 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
13/04/04 02:07:40 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
13/04/04 02:07:41 INFO common.Storage: Storage directory /home/gpuser/data/namenode has been successfully formatted.
13/04/04 02:07:41 INFO namenode.FSImage: Saving image file /home/gpuser/data/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
13/04/04 02:07:41 INFO namenode.FSImage: Image file of size 121 saved in 0 seconds.
13/04/04 02:07:41 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0
13/04/04 02:07:41 INFO util.ExitUtil: Exiting with status 0
13/04/04 02:07:41 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************
 
gpuser@ubuntu:~/hadoop-2.0.3-alpha$
                    </pre>
                </div>

                <p><strong>Start Hadoop File System</strong></p>
                <p>Start Namenode</p>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~/hadoop-2.0.3-alpha$ sbin/hadoop-daemon.sh start namenode
starting namenode, logging to /home/gpuser/hadoop-2.0.3-alpha/logs/hadoop-gpuser-namenode-ubuntu.out
                    </pre>                    
                </div>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~/hadoop-2.0.3-alpha$ sbin/hadoop-daemon.sh start datanode
starting datanode, logging to /home/gpuser/hadoop-2.0.3-alpha/logs/hadoop-gpuser-datanode-ubuntu.out
                    </pre>                    
                </div>
                <p>Verify that the Namenode and Datanode are running using jps</p>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~/hadoop-2.0.3-alpha$ jps
18509 Jps
17107 NameNode
17170 DataNode
                    </pre>                    
                </div>

                <p><strong>Start Yarn Daemons</strong></p>
                <p>Start Resource Manager</p>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~/hadoop-2.0.3-alpha$ sbin/yarn-daemon.sh start resourcemanager
starting resourcemanager, logging to /home/gpuser/hadoop-2.0.3-alpha/logs/yarn-gpuser-resourcemanager-ubuntu.out
gpuser@ubuntu:~/hadoop-2.0.3-alpha$ jps
18509 Jps
17107 NameNode
17170 DataNode
17252 ResourceManager
                    </pre>                    
                </div>
                <p>Start Node Manager</p>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~/hadoop-2.0.3-alpha$ sbin/yarn-daemon.sh start nodemanager
starting nodemanager, logging to /home/gpuser/hadoop-2.0.3-alpha/logs/yarn-gpuser-nodemanager-ubuntu.out
                    </pre>                    
                </div>

                <div class="codebox">
                    <pre>
gpuser@ubuntu:~/hadoop-2.0.3-alpha$jps
18509 Jps
17107 NameNode
17170 DataNode
17252 ResourceManager
17309 NodeManager
                    </pre>                    
                </div>

                <p>Start Job History Server</p>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~/hadoop-2.0.3-alpha$ sbin/mr-jobhistory-daemon.sh start historyserver
 
starting historyserver, logging to /home/gpuser/hadoop-2.0.3-alpha/logs/mapred-gpuser-historyserver-ubuntu.out
gpuser@ubuntu:~/hadoop-2.0.3-alpha$jps
18509 Jps
17107 NameNode
17170 DataNode
17252 ResourceManager
17309 NodeManager
17626 JobHistoryServer
                    </pre>                    
                </div>
                <p><strong>Verify Hadoop installation</strong></p>
                <p>With Web Interfaces.</p>
                <p>Open the browser with the following URL’s</p>
                <div class="codebox">
                    <pre>
HDFS-UI: http://localhost:50070
Resource Mnager UI: http://localhost:8088
Job History UI: http://localhost:19888
                    </pre>                    
                </div>

                <p>Run the WordCount example</p>
                <p>Verify the installation by running the Wordcount Example. This is an example to count the number of times, each word appears in the given input data set.</p>
                <p>Create input dir and create a sample file ‘animals.txt’ with the following content. Use your favoirite editor vi or emacs or gedit.</p>

                <div class="codebox">
                    <pre>
cat dog elephant zebra wolf
cat dog elephant zebra wolf
cat dog elephant zebra wolf
cat dog elephant zebra wolf
                    </pre>                    
                </div>

                <p>Copy the animals.txt file from local filesystem to a ‘/input’ file in hadoop filesystem using</p>

                <div class="codebox">
                    <pre>
gpuser@ubuntu:~/hadoop-2.0.3-alpha$ bin/hadoop dfs -copyFromLocal \
      input/animals.txt /input
                    </pre>                    
                </div>

                <p>Run the wordcount MapReduce program</p>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~/hadoop-2.0.3-alpha$ bin/hadoop jar \
        share/hadoop/mapreduce/hadoop-mapreduce-examples-2.*-alpha.jar
        wordcount /input /output
 
gpuser@ubuntu:~/hadoop-2.0.3-alpha$ bin/hadoop dfs -ls /
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.
 
Found 3 items
drwxr-xr-x   - gpuser supergroup          0 2013-04-04 02:44 /output
-rw-r--r--   1 gpuser supergroup        129 2013-04-04 02:43 /input
drwxrwx---   - gpuser supergroup          0 2013-04-04 02:42 /tmp
                    </pre>                    
                </div>

                <p>Use dfs cat command to see the output</p>
                <div class="codebox">
                    <pre>
gpuser@ubuntu:~/hadoop-2.0.3-alpha$ bin/hadoop dfs -cat /output/*
cat 4
dog 4
elephant 4
zebra 4
wolf 4
                    </pre>                    
                </div>
            </div><!--/single node-->
                <hr />

            <div id="multi-nodeInstallation" style="display:block;margin-top:-40px;"><br><br>
                <h1>Multi-node installation</h1>
                <hr />
                <p>Running Hadoop 2.0.3-alpha On Ubuntu Linux – Multi Node Cluster</p>
                <p>The goal of this tutorial is to run Hadoop 2.x on a multi-node cluster. For this exercise, we would be using two Ubuntu Linux boxes.</p>
                <p><strong>Prerequisities</strong></p>
                <p>Two Linux boxes with Ubuntu 12.* or One Linux box and one VM on the same box</p>
                <p>Running Hadoop Version 2 – Single Node cluster. Complete<a href=""> Singlenode Hadoop installation </a>tutorial before proceeding further.</p>
                <p>This tutorial has been tested with the following software versions:</p>
                <p><strong>Ubuntu Linux 12.04 LTS</strong></p><br />
                <p><strong>JDK 1.7</strong></p>
                <p>Through out the tutorial,commands to be executed on the master and slave node are mentioned as follows:</p>
                <p><strong>master</strong> – execute commands on the master node</p>
                <p><strong>slave</strong> – execute commands on the slave node</p>
                <h3>Overview</h3>
                <p>The following picture gives a brief overview of the cluster. Two Node Cluster</p>
                <img src="ddf/sds.png">
                <p>FIgure shows a two node cluster. Since, we have only two nodes, we will use the master node as both master and slave. For clarity, master and slave1 are shown as separate nodes. However, in this tutorial,master will also act as one of the slave.</p><br />
                <p>Master node runs both the Namenode and ResourceManager.Since Master is also a slave, NodeManger and DataNode also runs on the master node.</p>
                <p><strong>Master</strong></p>
                <ul>
                    <li>NameNode</li>
                    <li>Resource Manager</li>
                    <li>NodeManager</li>
                    <li>HstoryServer</li>
                    <li>DataNode</li>
                    <li>slave1</li>
                </ul>
                <p><strong>Data Node</strong></p>
                <ul>
                    <li>Node Manager</li>
                    <li>Steps for Installation</li>
                </ul>

                <p><strong>Step 1:</strong>Preparing the machines for the Cluster</p>
                <p>Install the operating system on both the machines. The machines should be able to ping each other. Check the IP addresses of the machines. If dhcp is used, make sure the ip-address does not change for the rest of the tutorial.</p>
                <p>Note the IP addresses. The IP address can be obtained by using the command</p>

                <div class="codebox">
                    <pre>
$ sudo ifconfig eth0
                    </pre>
                </div>
                <p>master – 172.16.109.1</p>
                <p>slave – 172.16.109.128</p>
                <p><strong>*Note: IP addresses in your system may be different</strong></p>
                <p>Edit /etc/hosts on the both master and slave. Add the following lines in /etc/hosts file</p>


                <div class="codebox">
                    <pre>
localhost 127.0.0.1
172.16.109.1 master
172.16.109.128 slave
                    </pre>
                </div>
                <p>Make sure other entries are removed or commented in the/etc/hosts file. Verify that both machines ping each other “`</p>
                <p><strong>master</strong> – Ping slave from master</p>
                <p>From the master machine issue the following command</p>
                <div class="codebox">
                    <pre>
gpuser@master$ping slave
                    </pre>
                </div>

                <p><strong>slave </strong>– Ping master from slave</p>
                <p>From the slave machine issue the following command</p>
                <div class="codebox">
                    <pre>
gpuser@slave$ping master
                    </pre>
                </div>

                <p>The ping should be successful, if not contact your IT support to fix any networking issues.</p>
                <p><strong>Step 2:</strong> Creating dedicated user and storage directories</p>
                <p>Create user gpuser(home dir /home/gpuser) and group gp on both master and slave</p>
                <p>Create the following directories on namenode and masternode under home directory /home/gpuser</p>
                <p><strong>master</strong></p>

                <div class="codebox">
                    <pre>
$mkdir -p data/namenode
$mkdir -p data/datanode
                    </pre>
                </div>
                <p><strong>slave</strong></p>
                <div class="codebox">
                    <pre>
mkdir -p data/datanode
                    </pre>
                </div>

                <p><strong>Step 3: </strong>Install ssh and set up password less ssh between master and slave</p>
                <p><strong>master</strong></p>
                <p><strong>Install SSH</strong></p>
                <div class="codebox">
                    <pre>
gpuser@master:~$sudo apt-get install ssh
gpuser@master:~$sudo apt-get install rsync
                    </pre>
                </div>

                <p>Generate public/private RSA key pairs using the following command.</p>
                <div class="codebox">
                    <pre>
gpuser@master:~$ ssh-keygen -t rsa -P ""
Generating public/private rsa key pair.
Enter file in which to save the key (/home/gpuser/.ssh/id_rsa):
Created directory '/home/gpuser/.ssh'.
Your identification has been saved in /home/gpuser/.ssh/id_rsa.
Your public key has been saved in /home/gpuser/.ssh/id_rsa.pub.
The key fingerprint is:
3c:f7:ca:f2:49:a7:05:ee:90:66:05:0f:7b:a9:63:fc gpuser@master
The key's randomart image is:
+--[ RSA 2048]----+
| |
| |
| o |
| . = . |
| S B |
| . O o |
| X o + |
| +.B * |
| oE |
+-----------------+
                    </pre>
                </div>
                <p>Press Enter to save the key in the default location.</p>
                <p>Enable ssh to the local machine using the following command.</p>
                <div class="codebox">
                    <pre>
gpuser@master:~$ cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
                    </pre>
                </div>

                <p>Test the ssh setup by connecting to the local machine with the gpuser user.</p>
                <div class="codebox">
                    <pre>
gpuser@master:~$ ssh localhost
 
The authenticity of host 'localhost (127.0.0.1)' can't be established.
ECDSA key fingerprint is ca:96:1c:5a:38:f8:9f:99:45:d4:57:82:3c:1b:64:b7.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
 
The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
                    </pre>
                </div>

                <p><strong>slave</strong></p>
                <p>repeat the above steps on slave</p>
                <p>Authorize master to allow ssh on slave node without the password</p>
                <p><strong>master</strong></p>
                <div class="codebox">
                    <pre>
gpuser@master~$ ssh-copy-id -i $HOME/.ssh/id_rsa.pub gpuser@slave
                    </pre>
                </div>

                <p>Execute the command ssh slave to verify ssh to slave works without the password</p>
                <div class="codebox">
                    <pre>
gpuser@master:~$ ssh slave
Welcome to Ubuntu 12.10 (GNU/Linux 3.5.0-17-generic i686)
 
* Documentation: https://help.ubuntu.com/
 
329 packages can be updated.
98 updates are security updates.
 
Last login: Fri Apr 5 00:32:00 2013 from ip6-localhost
gpuser@slave:~$
#exit from the ssh
gpuser@slave:~$ exit
                    </pre>
                </div>

                <p><strong>Step 4:</strong> Install Hadoop on Master</p>
                <p>Follow the steps as outlined in the tutorial ‘Running Hadoop Version 2 – Single Node cluster’ and complete the installation of Hadoop on master.</p>
                <p><strong>Step 5: </strong>Master node configuration</p>
                <p>Set the following environement variables in .bashrc</p>
                <p>Open .bashrc file in the home folder and add the following lines at the end</p>
                <div class="codebox">
                    <pre>
export HADOOP_HOME=$HOME/hadoop-2.0.3-alpha export HADOOP_MAPRED_HOME=$HOME/hadoop-2.0.3-alpha export HADOOP_COMMON_HOME=$HOME/hadoop-2.0.3-alpha export HADOOP_HDFS_HOME=$HOME/hadoop-2.0.3-alpha export YARN_HOME=$HOME/hadoop-2.0.3-alpha export HADOOP_CONF_DIR=$HOME/hadoop-2.0.3-alpha/etc/hadoop export JAVA_HOME=$HOME/java/jdk1.7.0_17 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin
                    </pre>
                </div>

                <p>Source the variables using the following command</p>
                <div class="codebox">
                    <pre>
$ source ~/.bashrc
                    </pre>
                </div>

                <p>Search for JAVA_HOME and set export JAVA_HOME variable in libexec/hadoop-config.sh</p>
                <div class="codebox">
                    <pre>
export JAVA_HOME=$HOME/java/jdk1.7.0_17
                    </pre>
                </div>

                <p>Add following lines at start of script in etc/hadoop/yarn-env.sh :</p>
                <div class="codebox">
                    <pre>
export JAVA_HOME=$HOME/java/jdk1.7.0_17
export HADOOP_HOME=/home/hadoop-2.0.3-alpha
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
                    </pre>
                </div>

                <p>Hadoop configration files at located at $HADOOP_HOME/etc/hadoop.</p>
                <p>Update the configuration files with the following entries respectively.</p>
                <div class="codebox">
                    <pre>
$HADOOP_HOME/etc/hadoop/master
                    </pre>
                </div>

                <p>Open master file and add the following line</p>
                <div class="codebox">
                    <pre>
master
                    </pre>
                </div>

                <p>$HADOOP_HOME/etc/hadoop/slaves</p>
                <p>Open slaves file and add the following lines</p>
                <div class="codebox">
                    <pre>
master
slave
                    </pre>
                </div>

                <p>*Note that the master is also listed as one the slave.</p>
                <p>Open the file and copy the following contents respectively.</p>
                <p><strong>$HADOOP_HOME/etc/hadoop/core-site.xml</strong></p>
                <div class="codebox">
                    <pre>
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;fs.default.name&lt;/name&gt;
         &lt;value&gt;hdfs://master:9000&lt;/value&gt;
     &lt;/property&gt;
&lt;/configuration&gt;
                    </pre>
                </div>

                <p><strong>$HADOOP_HOME/etc/hadoop/hdfs-site.xml</strong></p>
                <div class="codebox">
                    <pre>
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
 &lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;dfs.replication&lt;/name&gt;
         &lt;value&gt;2&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
         &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
         &lt;value&gt;file:/home/gpuser/data/namenode&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
         &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
         &lt;value&gt;file:/home/gpuser/data/datanode&lt;/value&gt;
     &lt;/property&gt;
 &lt;/configuration&gt;
                    </pre>
                </div>

                <p><strong>$HADOOP_HOME/etc/hadoop/mapred-site.xml</strong></p>
                <div class="codebox">
                    <pre>
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
         &lt;value&gt;yarn&lt;/value&gt;
     &lt;/property&gt;
&lt;/configuration&gt;
                    </pre>
                </div>

                <p><strong>$HADOOP_HOME/yarn-site.xml :</strong></p>
                <div class="codebox">
                    <pre>
&lt;?xml version=&quot;1.0&quot;?&gt;
 &lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
         &lt;value&gt;mapreduce.shuffle&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
         &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
         &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
         &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
         &lt;value&gt;master:8030&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
         &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
         &lt;value&gt;master:8031&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
         &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
         &lt;value&gt;master:8032&lt;/value&gt;
     &lt;/property&gt;
 &lt;/configuration&gt;
                    </pre>
                </div>

                <p><strong>Step 6: Install hadoop on Slave node</strong></p>
                <p><strong>master</strong></p>
                <p>Use the following command to copy hadoop installation from master to the slave</p>
                <div class="codebox">
                    <pre>
gpuser@master:~$ scp -r hadoop-2.0.3-alpha gpuser@slave:/home/gpuser
                    </pre>
                </div>

                <p><strong>slave</strong></p>
                <p>Verify that the following folder exists on slave.</p>
                <div class="codebox">
                    <pre>
gpuser@slave:~$ ls -al hadoop-2.0.3-alpha
                    </pre>
                </div>

                <p><strong>Step 7: Slave node configuration</strong></p>
                <p>Open .bashrc file in the home folder and add the following lines at the end</p>
                <div class="codebox">
                    <pre>
export HADOOP_HOME=$HOME/hadoop-2.0.3-alpha
export HADOOP_MAPRED_HOME=$HOME/hadoop-2.0.3-alpha
export HADOOP_COMMON_HOME=$HOME/hadoop-2.0.3-alpha
export HADOOP_HDFS_HOME=$HOME/hadoop-2.0.3-alpha
export YARN_HOME=$HOME/hadoop-2.0.3-alpha
export HADOOP_CONF_DIR=$HOME/hadoop-2.0.3-alpha/etc/hadoop
export JAVA_HOME=$HOME/java/jdk1.7.0_17
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin
                    </pre>
                </div>

                <p>Source the variables using the following command</p>
                <div class="codebox">
                    <pre>
$ source ~/.bashrc
                    </pre>
                </div>

                <p>Search for JAVA_HOME and set export JAVA_HOME variable in libexec/hadoop-config.sh</p>
                <div class="codebox">
                    <pre>
export JAVA_HOME=$HOME/java/jdk1.7.0_17
                    </pre>
                </div>

                <p>Search for JAVA_HOME and set export JAVA_HOME variable in etc/hadoop/yarn-env.sh</p>
                <div class="codebox">
                    <pre>
export JAVA_HOME=$HOME/java/jdk1.7.0_17
                    </pre>
                </div>

                <p>Remove the file $HADOOP_HOME/etc/hadoop/slaves</p>
                <p><strong>slave</strong></p>
                <div class="codebox">
                    <pre>
rm $HADOOP_HOME/etc/hadoop/slaves
                    </pre>
                </div>

                <p>open the file and copy the following contents respectively</p>
                <p><strong>yarn-site.xml</strong></p>
                <div class="codebox">
                    <pre>
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
         &lt;value&gt;mapreduce.shuffle&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
          &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
          &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
          &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
          &lt;value&gt;master:8030&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
          &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
          &lt;value&gt;master:8031&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
          &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
          &lt;value&gt;master:8032&lt;/value&gt;
     &lt;/property&gt;
&lt;/configuration&gt;

                    </pre>
                </div>

                <p><strong>core-site.xml</strong></p>
                <div class="codebox">
                    <pre>
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;fs.default.name&lt;/name&gt;
         &lt;value&gt;hdfs://master:9000&lt;/value&gt;
     &lt;/property&gt;
&lt;/configuration&gt;
                    </pre>
                </div>

                <p><strong>hdfs-site.xml</strong></p>
                <div class="codebox">
                    <pre>
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;dfs.replication&lt;/name&gt;
         &lt;value&gt;1&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
         &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
         &lt;value&gt;file:/home/gpuser/data/namenode&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
          &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
           &lt;value&gt;file:/home/gpuser/data/datanode&lt;/value&gt;
     &lt;/property&gt;
  &lt;/configuration&gt;
                    </pre>
                </div>

                <p><strong>mapred-site.xml</strong></p>
                <div class="codebox">
                    <pre>
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;

                    </pre>
                </div>

                <p><strong>Step 8:</strong> Initialize Hadoop File System</p>
                <p>Namenode needs to be initialized before starting with Hadoop File System. Namenode creates a unique namespace id for this instance of the Hadoop File System.</p>
                <div class="codebox">
                    <pre>
gpuser@master:~/hadoop-2.0.3-alpha$ bin/hadoop namenode -format

Formatting using clusterid: CID-8b844021-d1ea-4b0a-a625-d17dcc133299
13/04/04 02:07:39 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
13/04/04 02:07:39 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
13/04/04 02:07:39 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
13/04/04 02:07:39 INFO blockmanagement.BlockManager: defaultReplication = 1
13/04/04 02:07:39 INFO blockmanagement.BlockManager: maxReplication = 512
13/04/04 02:07:39 INFO blockmanagement.BlockManager: minReplication = 1
13/04/04 02:07:39 INFO blockmanagement.BlockManager: maxReplicationStreams = 2
13/04/04 02:07:39 INFO blockmanagement.BlockManager: shouldCheckForEnoughRacks = false
13/04/04 02:07:39 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
13/04/04 02:07:39 INFO blockmanagement.BlockManager: encryptDataTransfer = false
13/04/04 02:07:39 INFO namenode.FSNamesystem: fsOwner = gpuser (auth:SIMPLE)
13/04/04 02:07:39 INFO namenode.FSNamesystem: supergroup = supergroup
13/04/04 02:07:39 INFO namenode.FSNamesystem: isPermissionEnabled = true
13/04/04 02:07:39 INFO namenode.FSNamesystem: HA Enabled: false
13/04/04 02:07:39 INFO namenode.FSNamesystem: Append Enabled: true
13/04/04 02:07:40 INFO namenode.NameNode: Caching file names occuring more than 10 times
13/04/04 02:07:40 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
13/04/04 02:07:40 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
13/04/04 02:07:40 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension = 30000
13/04/04 02:07:41 INFO common.Storage: Storage directory /home/gpuser/data/namenode has been successfully formatted.
13/04/04 02:07:41 INFO namenode.FSImage: Saving image file /home/gpuser/data/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
13/04/04 02:07:41 INFO namenode.FSImage: Image file of size 121 saved in 0 seconds.
13/04/04 02:07:41 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0
13/04/04 02:07:41 INFO util.ExitUtil: Exiting with status 0
13/04/04 02:07:41 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************
gpuser@master:~/hadoop-2.0.3-alpha$
                    </pre>
                </div>

                <p><strong>Step 9:</strong> Start HDFS on Master Node</p>
                <div class="codebox">
                    <pre>
$ sbin/hadoop-daemon.sh start namenode
$ sbin/hadoop-daemons.sh start datanode
                    </pre>
                </div>

                <p>Verify namenode and data node running on master and slave</p>
                <p>Check namenode and datanode running using the following command</p>
                <p><strong>master</strong></p>
                <div class="codebox">
                    <pre>
$ jps
19785 DataNode
24567 Jps
16407 NameNode

                    </pre>
                </div>

                <p><strong>slave</strong></p>
                <p>Check datanode running using the following command</p>
                <div class="codebox">
                    <pre>
$ jps
13589 DataNode
15203 Jps
                    </pre>
                </div>

                <p><strong>Step 10:</strong> Start Yarn Services</p>
                <p>Run the following commands on the master node</p>
                <div class="codebox">
                    <pre>
$ sbin/yarn-daemon.sh start resourcemanager
$ sbin/yarn-daemons.sh start nodemanager
$ sbin/mr-jobhistory-daemon.sh start historyserver
                    </pre>
                </div>

                <p>Verify yarn services running in master node</p>
                <p><strong>master</strong></p>
                <div class="codebox">
                    <pre>
$ jps
19785 DataNode
24567 Jps
16407 NameNode
16409 ResourceManager
16410 NodeManager
16542 HistoryServer
                    </pre>
                </div>

                <p>Verify yarn services running in slave node</p>
                <p><strong>slave</strong></p>
                <div class="codebox">
                    <pre>
$ jps
13589 DataNode
13520 NodeManager
15203 Jps
                    </pre>
                </div>

                <p>Verify Hadoop installation</p>
                <p>With Web Interfaces.</p>
                <p>Open the browser with the following URL’s</p>
                <p>HDFS-UI: <a href="http://localhost:50070 HDFS-UI">http://localhost:50070 HDFS-UI</a>
                Resource Mnager UI: <a href="http://localhost:8088 ResourceManager">http://localhost:8088 ResourceManager</a>
                Job History UI: <a href="http://localhost:19888 NodeManager">http://localhost:19888 NodeManager</a>
                Run the WordCount example</p>
                <p>Verify the installation by running the Wordcount Example. This is an example to count the number of times, each word appears in the given input data set.
                Create input dir and create a sample file ‘animals.txt’ with the following content. Use your favoirite editor vi or emacs or gedit.</p>
                <div class="codebox">
                    <pre>
cat dog elephant zebra wolf
cat dog elephant zebra wolf
cat dog elephant zebra wolf
cat dog elephant zebra wolf
                    </pre>
                </div>

                <p>Copy the animals.txt file from local filesystem to a ‘/input’ file in hadoop filesystem using</p>
                <div class="codebox">
                    <pre>
gpuser@master:~/hadoop-2.0.3-alpha$ bin/hadoop dfs -copyFromLocal /home/animals.txt /input
                    </pre>
                </div>

                <p>Run the wordcount MapReduce program</p>
                <div class="codebox">
                    <pre>
gpuser@master:~/hadoop-2.0.3-alpha$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.*-alpha.jar wordcount /input /output

gpuser@master:~/hadoop-2.0.3-alpha$ bin/hadoop dfs -ls /
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

Found 3 items
drwxr-xr-x - gpuser supergroup 0 2013-04-04 02:44 /output
-rw-r--r-- 1 gpuser supergroup 129 2013-04-04 02:43 /input
drwxrwx--- - gpuser supergroup 0 2013-04-04 02:42 /tmp
                    </pre>
                </div>

                <p>Use dfs cat command to see the output</p>
                <div class="codebox">
                    <pre>
gpuser@master:~/hadoop-2.0.3-alpha$ bin/hadoop dfs -cat /output/*
cat 4
dog 4
elephant 4
zebra 4
wolf 4
                    </pre>
                </div>
                <hr />
            </div><!--/Multinode-->


        <div id="hdfsIntro" style="display:block;margin-top:-40px;"><br><br>
            <h2>HDFS</h2>
            <hr />
            <p>Agenda of the tutorial is to give the insights to Hadoop Distributed File System.</p>
            <p><strong>Introduction to HDFS</strong></p>
            <p>The foundation of efficient data processing in Hadoop is its data storage model.HDFS is Hadoop’s implementation of a distributed filesystem. It is designed to hold a large amount of data, and provide access to this data to many clients distributed across a network.To be able to successfully leverage HDFS, you first must understand how it is implemented and how it works.</p>
            <p><strong>The Design of HDFS</strong></p>
            <p>The HDFS design is based on the design of the Google File System (GFS).</p>
            <p>HDFS is a filesystem designed for storing very large files with streaming data access
            patterns, running on clusters of commodity hardware.</p>
            <p><strong>Very large files</strong></p>
            <p>“Very large” in this context means files that are hundreds of megabytes, gigabytes,
            or terabytes in size.</p>
            <p>Streaming data access</p>
            <p>HDFS uses &#8220;write-once, read-many-times&#8221; pattern .Over the time we are performing several operation on data which is once copied to HDFS.In this case the total amout of time took to access whole data is more important than the latency in reading the first record.</p>
            <p><strong>Commadity hardware</strong></p>
            <p>Hadoop can run on commonly available hardware.</p>
            <p><strong>Low-latency data access</strong></p>
            <p>Applications that require low-latency access to data, in the tens of milliseconds
            range, will not work well with HDFS.</p>
            <p><strong>Lots of small files</strong></p>
            <p>number of files in a filesystem is governed by the amount of memory on the namenode. As a rule of thumb, each file, directory, and block takes about 150 bytes. So, for example, if you had one million files, each taking one block, you would need at least 300 MB of memory. While storing millions of files is feasible, billions is beyond the capability of current hardware</p>
            <p><strong>Multiple writers,arbitary file modifications</strong></p>
            <p>Files in <strong>HDFS</strong> may be written to by a single writer. Writes are always made at the
            end of the file.</p>
            <p><strong>HDFS Concepts</strong></p>
            <p><strong>Blocks</strong></p>

            <p><a href="http://www.datascience-labs.com/wp-content/uploads/2013/11/hdfs-blocks.png"><img class="size-medium wp-image-220 alignleft" alt="hdfs-blocks" src="http://www.datascience-labs.com/wp-content/uploads/2013/11/hdfs-blocks-300x139.png" width="300" height="139" /></a>A block size is the minimum amout of data that can be read or write in to disk.</p>
            <p>File system blocks are typically a few killo bytes in size.</p>
            <p>HDFS, too, has the concept of a block, but it is a much larger unit—64 MB by default.</p>
            <p>Files in HDFS are broken into block-sized chunks,which are stored as independent units.</p>
            <p>HDFS block size is large to minimize the cost of seeks.</p>
            <p>Advantages of block abstraction:</p>
            <p>1)A file can be larger than any single disk in network.</p>
            <p>2)Since unit of abstraction a block rather than a file  simplifies storage subsystem.</p>
            <p>3)Blocks fit well with replication for providing fault tolerant</p>
            <p><strong>Namenodes and Datanodes</strong></p>
            <p><a href="http://www.datascience-labs.com/wp-content/uploads/2013/11/dfs-1-hdfs-chunks.png"><img class="size-medium wp-image-221 alignleft" alt="dfs-1-hdfs-chunks" src="http://www.datascience-labs.com/wp-content/uploads/2013/11/dfs-1-hdfs-chunks-300x199.png" width="300" height="199" /></a></p>
            <p>Namenode manages the filesystem name space.</p>
            <p>Namenode maintains filesystem tree and metadata of all the files in directories in the tree.This information is stored in <strong>namespace image</strong> and <strong>edit log.</strong></p>
            <p><strong>Data nodes</strong> are work horses of file system.They store and retrieve blocks when they are told by client.Data nodes report back to name node periodically with list of blocks that they are storing.</p>
            <p><strong>Secondary namenode</strong> periodically merge the name space image with edit log to prevent edit log from becoming too large.</p>
            <p>The secondary namenode usually runs on a separate physical machine</p>
            <p>It keeps a copy of the merged name space image, which can be used in the event of the namenode failing.</p>
            <p>In case of total failure of the primary, data loss is almost certain.
            we can only copy the namenode’s metadata files that are on NFS to the secondary and run it as the new primary.</p>
            <p><strong>HDFS fedaration </strong></p>
            <p>Namenode keeps log of every file and block in the  filesystem in memory.So the memory available on namenode is the limiting factor for scaling.</p>
            <p>Scaling Namenode can be achieved by HDFS fedaration.</p>
            <p><a href="http://www.datascience-labs.com/wp-content/uploads/2013/11/federation.gif"><img class="alignnone size-medium wp-image-232" alt="federation" src="http://www.datascience-labs.com/wp-content/uploads/2013/11/federation-300x180.gif" width="300" height="180" /></a></p>
            <p>In federation, each namenode manages a <strong>namespace volume</strong>, which is made up of
            the metadata for the namespace, and a <strong>block pool</strong> containing all the blocks for the files
            in the namespace.</p>
            <p>Namespace volumes are independent of each other,  furthermore the failure of one
            namenode does not affect the availability of the namespaces managed by other namen-
            odes.</p>
            <p><strong>HDFS High Availability</strong></p>
            <ul>
            <li><a href="http://blog.cloudera.com/blog/2012/03/high-availability-for-the-hadoop-distributed-file-system-hdfs/">High availability for the HDFS NameNode</a>, which eliminates the previous SPOF in HDFS.</li>
            <li>Support for filesystem snapshots in HDFS, which brings native backup and disaster recovery processes to Hadoop.</li>
            <li>Support for federated NameNodes, which allows for horizontal scaling of the filesystem namespace.</li>
            <li>Support for NFS access to HDFS, which allows HDFS to be mounted as a standard filesystem.</li>
            </ul>
            <hr />
        </div>


        <div id="hdfsCommands" style="display:block;margin-top:-40px;"><br><br>
            <h2>Try HDFS Commands</h2>
            <p>Hadoop provides several ways of accessing HDFS. The FileSystem (FS) shell
            commands provide a wealth of operations supporting access and manipulation
            of HDFS files. These operations include viewing HDFS directories,creating files,deleting iles,copying files, and so on.</p>
            <p>In this document we will try various HDFS Commands. Practising these commands helps one to understand and use Hadoop better.</p>

            <p><strong>hadoop fs help information</strong></p>
                <div class="codebox">
                    <pre>
[esammer@hadoop01 ~]$ hadoop fs
Usage: java FsShell
[-ls &lt;path&gt;]
[-lsr &lt;path&gt;]
[-df [&lt;path&gt;]]
[-du &lt;path&gt;]
[-dus &lt;path&gt;]
[-count[-q] &lt;path&gt;]
[-mv &lt;src&gt; &lt;dst&gt;]
[-cp &lt;src&gt; &lt;dst&gt;]
[-rm [-skipTrash] &lt;path&gt;]
[-rmr [-skipTrash] &lt;path&gt;]
[-expunge]
[-put &lt;localsrc&gt; &#8230; &lt;dst&gt;]
[-copyFromLocal &lt;localsrc&gt; &#8230; &lt;dst&gt;]
[-moveFromLocal &lt;localsrc&gt; &#8230; &lt;dst&gt;]
[-get [-ignoreCrc] [-crc] &lt;src&gt; &lt;localdst&gt;]
[-getmerge &lt;src&gt; &lt;localdst&gt; [addnl]]
[-cat &lt;src&gt;]
[-text &lt;src&gt;]
[-copyToLocal [-ignoreCrc] [-crc] &lt;src&gt; &lt;localdst&gt;]
[-moveToLocal [-crc] &lt;src&gt; &lt;localdst&gt;]
[-mkdir &lt;path&gt;]
[-setrep [-R] [-w] &lt;rep&gt; &lt;path/file&gt;]
[-touchz &lt;path&gt;]
[-test -[ezd] &lt;path&gt;]
[-stat [format] &lt;path&gt;]
[-tail [-f] &lt;file&gt;]
[-chmod [-R] &lt;MODE[,MODE]&#8230; | OCTALMODE&gt; PATH&#8230;]
[-chown [-R] [OWNER][:[GROUP]] PATH&#8230;]
[-chgrp [-R] GR
                    </pre>
                </div>

                <h2>Try the following commands</h2>
                <h3>Running Hadoop in Local Mode</h3>
                <p>If you already have configured hadoop to run in Psuedo mode or cluster mode, use the following file to point to the right
                hadoop cluster configuration</p>
                <p><configuration>
                    <property>
                    <name>fs.default.name</name><br />
                        <value>file:///</value>
                    </property>
                    <property>
                        <name>mapred.job.tracker</name><br />
                        <value>local</value>
                    </property>
                    </configuration>
                </p>
                <p>#Lists all the files in the local file system</p>
                <pre>
hadoop fs -conf /tmp/hadoop-local.xml  -ls /
                </pre>
                <p>#Lists all the files in the HDFS  file system</p>
                <pre>
hadoop fs -conf /tmp/hadoop-cluster.xml  -ls /
                </pre>

                <p>#Lists all the files in the $HADOOP_HOME/conf/hdfs-site.xml<br />
                #uses the configuration file found in the class path</p>
                <h3>Running Hadoop in Pusedo distributed Mode</h3>
                <ul>
                    <li>
                        <p>Hadoop must be running in Psuedo or cluster mode</p>
                        <pre>
hadoop fs  -ls /
bin/hadoop fs  -mkdir stocks
bin/hadoop fs -ls /user/hadoop/stocks
                        </pre>
                    </li>
                </ul>
                <p>#copy file from local file system to hadoop file system</p>
                <p>hadoop fs -copyFromLocal conf hdfs://localhost:9000/user/hadoop</p>
                <p>hadoop fs -copyFromLocal conf/* hdfs://localhost:9000/user/hadoop</p>
                <p>#copy the file to local diectory</p>
                <p>bin/hadoop dfs -copyToLocal /usr/hadoop/conf /conf</p>
                <p>#rm a directory</p>
                <p>bin/hadoop fs -rmr stocks</p>
                <p>#remove a filename</p>
                <p>bin/hadoop fs -rm filename</p>
                <p>#expunge empty the trash</p>
                <p>hadoop dfs -expunge</p>
                <p>#Get files to the local file system</p>
                <p>hadoop dfs -get /hadooptmp/input /tmp</p>
                <p>#cp from src to dest</p>
                <p>hadoop dfs -cp src dest</p>
                <p>#disk usage</p>
                <p>hadoop dfs -dus filename</p>
                <p>#Dir of files ; give the total size of the dir</p>
                <p>hadoop dfs -du dir</p>
                <p>#recirvsive version of ls</p>
                <p>bin/hadoop dfs -lsr /</p>
                <p>#disctp for copying files from one hdfs file system to the other</p>
                <p>hadoop distcp hdfs://namenode1://foo hdfs://namenode2/foo</p>
                <p>#skils if the file is already present use -overwrite if need to be overwirtten</p>
                <p>#betweeb two versions hdfs:// will not work; use http:// or webhdfs</p>
                <p>#move files from dir1 to dir2</p>
                <p>bin/hadoop dfs -mv /dir1 /dir2</p>
                <p>#copy from local to hdfs</p>
                <p>bin/hadoop dfs -put /hadooptmp/programming_hive.pdf</p>
                <p>#remove a file</p>
                <p>hadoop dfs rm filename</p>
                <p>#set replication factor to 3</p>
                <p>bin/hadoop dfs -setrep -w 3 -R /dir2</p>
                <p>#Returns the stat information on the path.</p>
                <p>bin/hadoop dfs -stat /dir2</p>
                <p>#test the existance of the file</p>
                <p>bin/hadoop dfs -test -[ezd] URI</p>
                <p>#create an emptyfile</p>
                <p>bin/hadoop -touchz pathname</p>
                <p>#output the fil in text format</p>
                <p>bin/hadoop dfs -text /input/ncdc/sample.txt</p>
                <p>#Displays last kilobyte of the file to stdout. -f option can be used as in Unix.</p>
                <p>bin/hadoop dfs -tail pathname</p>
                <p>#Adding new data nodes and tak tsacker nodes</p>
                <p>#add hostname to dfs.hosts</p>
                <p>#add hostname to mapred.hosts</p>
                <p>#adding a new node</p>
                <p>hadoop dfsadmin -refreshNodes</p>
                <p>#Adding a new task tracker</p>
                <p>hadoop mradmin -refreshNodes</p>
                <p>#Run the Java programs illustrating the API</p>
                <p>#Decommisioning a node</p>
                <p>#add hosts dfs.hosts.exclude</p>
                <p>#add hosts to mapred.hosts.exclude</p>
                <p>#ado this to start decommisioning process</p>
                <p>hadoop dfsadmin -refreshNodes</p>
                <p>#setting log levels</p>
                <p>hadoop daemonlog -setLevel jobtracker-host:port \org.apache.hadoop.mapred.JobTracker DEBUG</p>
                <p>#to make persistent log file</p>
                <p>#add the following line to log4j.properties</p>
                <p>log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG</p>
                <p>#cluster balancing</p>
                <p>hadoop balancer [-threshold ]</p>
                <p>hadoop dfs -chown [-R] user:group filenames</p>
                <p>#chmode</p>
                <p>hadoop dfs -chmode [-R] user:group filenames</p>
                <p>#hadoo archives</p>
                <p>% hadoop fs -lsr /my/files</p>
                <p>-rw-r--r-- drwxr-xr-x -rw-r--r--</p>
                <p>1 tom supergroup - tom supergroup 1 tom supergroup</p>
                <p>1 2009-04-09 19:13 /my/files/a</p>
                <p>0 2009-04-09 19:13 /my/files/dir</p>
                <p>1 2009-04-09 19:13 /my/files/dir/b</p>
                <p>This job produces a very small amount of output, so it is convenient to copy it from HDFS to our development machine. The -getmerge option to the hadoop fs command is useful here, as it gets all the files in the directory specified in the source pattern and merges them into a single file on the local filesystem:</p>
                <p>% hadoop fs -getmerge max-temp max-temp-local</p>
                <p>% sort max-temp-local | tail</p>
                <p>1991 607</p>
                <p>1992 605</p>
                <p>1993 567 1994 568 1995 567 1996 561 1997 565 1998 568 1999 568 2000 558</p>
                <hr />


        </div><!--/hdfs commands-->

        <div id="javaApi" style="display:block;margin-top:-40px;"><br><br>
            <h2>Java-API</h2>
            <p>Access to HDFS is through an instance of the <strong>FileSystem</strong> object. A <strong>FileSystem</strong> class is an abstract base class for a generic filesystem.</p>
            <p>We can perform the following operatins using Java -Api</p>
            <p><strong>The goals of this tutorial</strong></p>
            <p>1.HDFS Java API Introduction</p>
            <p>2.HDFS configuration</p>
            <p>3.Reading Data from HDFS</p>
            <p>4.Writing Data to HDFS</p>
            <p>5.Browsing file system</p>
            <p>HDFS Java API Introduction</p>
            <p><strong>org.apache.hadoop.fs.FileSystem</strong> is an abstract class that serves as a generic file system representation.<strong>org.apache.hadoop.fs.FileSystem</strong> is a class and not an Interface.</p>
            <p>Different types of File system Implementations</p>


            <p><strong> org.apache.hadoop.fs.LocalFileSystem</strong></p>
            <ul>
                <li>Good old native file system using local disk(s)</li>
            </ul>
            <p><strong>org.apache.hadoop.hdfs.DistributedFileSystem</strong></p>
            <ul>
                <li> Hadoop Distributed File System (HDFS)</li>
            </ul>
            <p><strong>org.apache.hadoop.hdfs.HftpFileSystem</strong></p>
            <ul>
                <li>Access HDFS in read-only mode over HTTP</li>
            </ul>
            <p><strong>org.apache.hadoop.fs.ftp.FTPFileSystem</strong></p>
            <ul>
                <li>File system on FTP server</li>
            </ul>
            <hr />
        </div>

        <div id="hiveIntro" style="display:block;margin-top:-40px;"><br><br>
            <h2>Hive - Introduction</h2>
                <p>Hadoop got a challenge in terms of moving existing data infrastructures to Hadoop ecosystem.</p>
                <p>Mapping SQL data operations to the low-level MapReduce Java API is a tedious task. Hive does this dirty work for you, so you can focus on the query itself. Hive translates most queries to MapReduce jobs, thereby exploiting the scalability of Hadoop, while presenting a familiar SQL abstraction.</p>
                <p>Hive is best suited for data warehouse applications, where a large data set is main-<br />
                tained and mined for insights, reports.</p>
                <p><strong>Hive Arichtecture</strong></p>
                <p style="text-align: left;"><a href="http://www.datascience-labs.com/wp-content/uploads/2013/11/hive_architecture.png"><img class="size-medium wp-image-281 aligncenter" style="border: 1px solid black;" alt="hive_architecture" src="http://www.datascience-labs.com/wp-content/uploads/2013/11/hive_architecture-275x300.png" width="275" height="300" /></a><strong>Metastore:</strong> stores system catalog</p>
                <p style="text-align: left;">To support features like schema(s) and data partitioning Hive keeps its metadata in a
                Relational Database.</p>
                <p style="text-align: left;">Meta store is packaged with Derby, a lightweight embedded SQL DB.</p>
                <p style="text-align: left;"><span style="line-height: 1.5;">Schema is not shared between users as each user has </span>their own instance of embedded Derby.</p>
                <p style="text-align: left;">Stored in <strong>metastore_db</strong> directory which resides in the directory that hive was started from</p>
                <p style="text-align: left;"><strong>Driver:</strong> manages life cycle of HiveQL query as it moves thru’ HIVE; also manages session handle and session statistics</p>
                <p style="text-align: left;"><strong>Query compiler:</strong> Compiles HiveQL into a directed acyclic graph of map/reduce tasks</p>
                <p style="text-align: left;"><strong>Execution engines:</strong> The component executes the tasks in proper dependency order; interacts with Hadoop</p>
                <p style="text-align: left;"><strong>HiveServer:</strong> provides Thrift interface and JDBC/ODBC for integrating other applications.</p>
                <p style="text-align: left;"><strong>Client components:</strong> CLI, web interface, jdbc/odbc inteface
                Extensibility interface include SerDe, User Defined Functions and User Defined Aggregate Function.</p>
        </div>

        <div id="hiveRunMode" style="display:block;margin-top:-40px;"><br><br>
            <h2>Hive – Run Modes</h2>
            <p><strong>Starting Hive</strong></p>
            <p>If you added $HIVE_HOME/bin to your PATH, you can just type hive to run the command.</p>
            <div class="codebox">
                <pre>
$ cd $HIVE_HOME
$ bin/hive
                </pre>
            </div>

            <p>Hive history file=/tmp/myname/hive_job_log_myname_201201271126_1992326118.txt hive</p>
            <div class="codebox">
                <pre>
&gt; CREATE TABLE x (a INT);
OK
Time taken: 3.543 seconds
hive&gt; SELECT * FROM x;
OK
Time taken: 0.231 seconds
hive&gt; SELECT *
&gt; FROM x; OK
Time taken: 0.072 seconds
hive&gt; DROP TABLE x;
OK
Time taken: 0.834 seconds
hive&gt; exit; $
                </pre>
            </div>

            <p><strong>Local Mode Configuration</strong></p>
            <p>Go to the $HIVE_HOME/conf directory. The curious may want to peek at the large hive-default.xml.template file, which shows the different configuration properties supported by Hive and their default values. Most of these properties you can safely ignore. Changes to your configuration are done by editing the hive-site.xml file. Create one if it doesn’t already exist.</p>
            <p>Here is an example configuration file where we set several properties for local mode execution.</p>
            <div class="codebox">
                <pre>
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt;
&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/home/me/hive/warehouse&lt;/value&gt; &lt;description&gt;
Local or HDFS directory where Hive keeps table contents.
&lt;/description&gt; &lt;/property&gt; &lt;property&gt;
&lt;name&gt;hive.metastore.local&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt; &lt;description&gt;
Use false if a production metastore server is used.
&lt;/description&gt; &lt;/property&gt; &lt;property&gt;
&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:derby:;databaseName=/home/me/hive/metastore_db;create=true&lt;/value&gt; &lt;description&gt;
The JDBC connection URL.
&lt;/description&gt; &lt;/property&gt;
&lt;/configuration&gt;
                </pre>
            </div>

            <p><strong>Distributed and Pseudodistributed Mode Configuration</strong></p>
            <p>In distributed mode, several services run in the cluster. The JobTracker manages jobs and the NameNode is the HDFS master. Worker nodes run individual job tasks, man- aged by a TaskTracker service on each node, and then hold blocks for files in the distributed filesystem, managed by DataNode services.</p>
            <p>One Hive property you might want to configure is the top-level directory for table storage, which is specified by the property hive.metastore.warehouse.dir ohe default value for this property is /user/hive/warehouse in the Apache Hadoop and MapR distributions, which will be interpreted as a distributed filesystem path when Hadoop is configured for distributed or pseudodistributed mode. For Amazon Elastic MapReduce (EMR), the default value is /mnt/hive0M_N/warehouse when using Hive v0.M.N (e.g., /mnt/hive08_1/warehouse).</p>
            <p>Specifying a different value here allows each user to define their own warehouse direc- tory, so they don’t affect other system users. Hence, each user might use the following statement to define their own warehouse directory:</p>
            <div class="codebox">
                <pre>
set hive.metastore.warehouse.dir=/user/myname/hive/warehouse;
                </pre>
            </div>

            <p><strong>Metastore Using JDBC</strong></p>
            <p>Hive requires only one extra component that Hadoop does not already have; the metastore component. The metastore stores metadata such as table schema and parti- tion information that you specify when you run commands such as create table x…, or alter table y…, etc. Because multiple users and systems are likely to need concurrent access to the metastore, the default embedded database is not suitable for production.</p>
            <p>Any JDBC-compliant database can be used for the metastore. In practice, most instal- lations of Hive use MySQL. We’ll discuss how to use MySQL. It is straightforward to adapt this information to other JDBC-compliant databases. For our MySQL configuration, we need to know the host and port the service is running on. We will assume db1.mydomain.pvt and port 3306, which is the standard MySQL port.</p>
            <div class="codebox">
                <pre>
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://db1.mydomain.pvt/hive_db?createDatabaseIfNotExist=true&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
&lt;/property&gt; &lt;property&gt;
&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
&lt;value&gt;database_user&lt;/value&gt; &lt;/property&gt;
&lt;property&gt;
&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
&lt;value&gt;database_pass&lt;/value&gt; &lt;/property&gt;
&lt;/configuration&gt;
                </pre>
            </div>
            <hr />
        </div>

        <div id="hiveBasic" style="display:block;margin-top:-40px;"><br><br>
            <h2>Hive Basics</h2>
            <h4>Primitive Data Types</h4>
            <p>There are the usual built-in data types. As always, case is ignored.</p>
            <table class="table table-dafault">
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Size</th>
                        <th colspan="2">Literal Syntax Examples</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>TINYINT</td>
                        <td>1-byte signed integer</td>
                        <td>20</td>
                    </tr>

                    <tr>
                        <td>SMALLINT</td>
                        <td>2-byte signed integer</td>
                        <td>20</td>
                    </tr>

                    <tr>
                        <td>INT</td>
                        <td>4-byte signed integer</td>
                        <td>20</td>
                    </tr>

                    <tr>
                        <td>BIGINT</td>
                        <td>8-byte signed integer</td>
                        <td>20</td>
                    </tr>

                    <tr>
                        <td>BOOLEAN</td>
                        <td>TRUE | FALSE</td>
                        <td>TRUE</td>
                    </tr>

                    <tr>
                        <td>FLOAT</td>
                        <td>4-byte single precision</td>
                        <td>3.14159</td>
                    </tr>

                    <tr>
                        <td>DOUBLE</td>
                        <td>8-byte double precision</td>
                        <td>3.14159</td>
                    </tr>

                    <tr>
                        <td>STRING</td>
                        <td>Sequence of characters. The character set can be specified.</td>
                        <td>‘Now is the time’, “for all good men”</td>
                    </tr>

                    <tr>
                        <td>TIMESTAMP</td>
                        <td>Integer, float or string</td>
                        <td>(Hive v0.8) 1327882394 (Unix epoch seconds), 1327882394.123456789 (Unix epoch seconds plus nanoseconds), and '2012-02-03 12:34:56.123456789' (JDBC-compliant java.sql.Timestamp format).</td>
                    </tr>

                    <tr>
                        <td>BINARY</td>
                        <td>Array of bytes</td>
                        <td>(Hive v0.8) see discussion below.</td>
                    </tr>

                </tbody>
            </table>

            <p>Note that these types reflect the underlying types provided by Java.</p>
            <p><strong>TIMESTAMP </strong>and <strong>BINARY </strong>are new to Hive v0.8.0. For Hive v0.7.X, use an integer value for the epoch seconds or use a <strong>STRING.</strong> </p>
            <p>If you use the latter, it is best to use a sortable format like the example shown.</p>
            <p>The <strong>BINARY</strong> type is a way of saying “ignore the rest of this record”; it is treated as a byte array without interpretation and with no built-in support for conversion to other types. Use it when you only care about the first few fields in a record, e.g.,</p>
            <div class="codebox">
                <pre>
CREATE TABLE short (s STRING, i INT, b BINARY);  
                </pre>
            </div>

            <p>When a query mentions a particular type, Hive will implicitly cast any integer type to a larger integer type, cast <strong>FLOAT</strong> to <strong>DOUBLE</strong>, and cast any integer type to <strong>DOUBLE</strong>, as needed.</p>
            <p>You can also explicitly interpret a <strong>STRING</strong> as a number type using, for example, '3.14159' to <strong>DOUBLE.</strong></p>
            <h4>Complex Data Types</h4>
            <p>Hive supports columns that are structs, maps, and arrays. Note that the “literal syntax examples” are actually making calls to built-in functions.</p>
            <table class="table table-default">
                <thead>
                    <tr>
                        <th>TYPE</th>
                        <th>DESCRIPTION</th>
                        <th>LITERAL SYNTAX EXAMPLES</th>

                    </tr>
                </thead>

                <tbody>
                    <tr>
                        <td>STRUCT</td>
                        <td>Analogous to a C struct or an “object”. Fields can be accessed using the “dot” notation. For example, if a column name if of type STRUCT {first STRING; last STRING}, then the first name field can be referenced using name.first.</td>
                        <td>struct('John', 'Doe')</td>
                    </tr>

                    <tr>
                        <td>MAP</td>
                        <td>A collection of key-value tuples, where the fields are accessed using array notation, e.g., [‘key’]. For example, if a column name is of type MAP with key->value pairs 'first'->'John' and 'last'->'Doe', then the last name can be referenced using name['last'].</td>
                        <td>map('first', 'John', 'last', 'Doe')</td>
                    </tr>

                    <tr>
                        <td>ARRAY</td>
                        <td>Lists of the same type that are indexable using zero-based integers. For example, if a column name is of type ARRAY of strings with the value ['John', 'Doe'], then the second element can be referenced using name[1].</td>
                        <td>array('John', 'Doe')</td>
                    </tr>

                </tbody>
            </table>

            <h4>File Formats</h4>
            <p>Hive supports all the Hadoop file formats, plus Thrift encoding, as well as supporting pluggable SerDe (serializer/deserializer) classes to support custom formats. Hive defaults to the following record and field delimiters, all of which are non-printable control characters and all of which can be customized.</p>

            <table class="table table-default">
                <thead>
                    <tr>
                        <th>DELIMITER</th>
                        <th>NAME</th>
                        <th>USE</th>

                    </tr>
                </thead>

                <tbody>
                    <tr>
                        <td>\n or \r</td>
                        <td>Line feed or carraige return</td>
                        <td>Records, i.e., one per line.</td>
                    </tr>

                    <tr>
                        <td>^A</td>
                        <td>Control-A (\001)</td>
                        <td>Separates fields, e.g., the way commas are used in CSV files.</td>
                    </tr>

                    <tr>
                        <td>^B</td>
                        <td>Control-B (\002)</td>
                        <td>Separates elements in the “collections” (STRUCT, MAP, and ARRAY)</td>
                    </tr>

                    <tr>
                        <td>^C</td>
                        <td>Control-C (\003)</td>
                        <td>Separates the key and value in MAP elements.</td>
                    </tr>

                </tbody>
            </table>

            <p>All of these defaults can be customized when creating tables. See examples below.</p>
            <p>There are several file formats supported by Hive. <strong>TEXTFILE</strong> is the easiest to use, but the least space efficient. Hadoop’s <strong>SEQUENCEFILE</strong> format is more space efficient. A related format is <strong>MAPFILE</strong> which adds an index to a <strong>SEQUENCEFILE</strong> for faster retrieval of particular records.</p>

        </div>

        <div id="hiveDataDefination" style="display:block;margin-top:-40px;"><br><br>
            <h2>HiveQL-Data Definition</h2>
            <p>HiveQL is the Hive query language. Like all SQL dialects in widespread use, it doesn’t fully conform to any particular revision of the ANSI SQL standard. Hive doesn’t support transactions. Hive adds ex-tensions to provide better performance in the context of Hadoop and to integrate with custom extensions and even external programs.</p>
            <p><strong>Prerequisites</strong></p>
            <p>Hadoop must be installed as per installation instructions <a href="">Hadoop version 2.0.3 installation</a>
            Hive must be installed as per installation instructions <a href="#hive/run-modes/">Hive version 0.10.0 installation</a>
            Load the dataset into HDFS using this <a href="">link</a>Tables Used In this tutorial</p>
            <ol>
                <li>Stocks</li>
                <li>Employees</li>
            </ol>
            <p><strong>Create database</strong></p>
            <p>They are very useful for larger clusters with multiple teams and users, as a way of avoiding table name collisions. It’s also common to use databases to organize production tables into logical groups. If you don’t specify a database, the default database is used.</p>
            <div class="codebox">
                <pre>
hive&gt; CREATE DATABASE IF NOT EXISTS financials;
                </pre>
            </div>

            <p>You can also use the keyword SCHEMA instead of DATABASE in all the database-related commands. At any time, you can see the databases that already exist as follows:</p>
            <div class="codebox">
                <pre>
hive&gt; SHOW DATABASES;
default
financials

hive&gt; CREATE DATABASE human_resources;

hive&gt; SHOW DATABASES;
default
financials
human_resources
                </pre>
            </div>

            <p>you can restrict the ones listed using a regular expression.</p>
            <div class="codebox">
                <pre>
hive&gt; SHOW DATABASES LIKE 'h.*';
human_resources
                </pre>
            </div>

            <p>DESCRIBE DATABASE shows the directory location for the database.</p>
            <div class="codebox">
                <pre>
hive&gt; DESCRIBE DATABASE financials;

hdfs://master-server/user/hive/warehouse/financials.db
                </pre>
            </div>

            <p>Hive will create a directory for each database. Tables in that database will be stored in subdirectories of the database directory. The exception is tables in the default database, which doesn’t have its own directory. The database directory is created under a top-level directory specified by the property hive.metastore.warehouse.dir. Assuming you are using the default value for this property, /user/hive/warehouse, when the financials database is created, Hive will create the directory /user/hive/warehouse/financials.db. Note the .db extension.</p>
            <p>You can override this default location for the new directory as shown in this example:</p>
            <div class="codebox">
                <pre>
hive&gt; CREATE DATABASE financials
&gt; LOCATION '/my/preferred/directory';
                </pre>
            </div>

            <p>USE database</p>
            <p>The USE command sets a database as your working database, analogous to changing working directories in a filesystem</p>
            <div class="codebox">
                <pre>
hive&gt; USE financials;
                </pre>
            </div>

            <p>We can set hive.cli.print.current.db property to print the current database as part of the prompt.</p>
            <div class="codebox">
                <pre>
hive&gt; set hive.cli.print.current.db=true;

hive (financials)&gt; USE default;

hive (default)&gt; set hive.cli.print.current.db=false;
                </pre>
            </div>

            <p>DROP database</p>
            <p>you can drop a database:</p>
            <div class="codebox">
                <pre>
hive&gt; DROP DATABASE IF EXISTS financials;
                </pre>
            </div>

            <p>The IF EXISTS is optional and suppresses warnings if financials doesn’t exist. By default, Hive won’t permit you to drop a database if it contains tables. You can either drop the tables first or append the CASCADE keyword to the command, which will cause the Hive to drop the tables in the database first:</p>
            <div class="codebox">
                <pre>
hive&gt; DROP DATABASE IF EXISTS financials CASCADE;
                </pre>
            </div>

            <p>Alter Database</p>
            <p>You can set key-value pairs in the DBPROPERTIES associated with a database using the ALTER DATABASE command. No other metadata about the database can be changed,including its name and directory location:</p>
            <div class="codebox">
                <pre>
hive&gt; ALTER DATABASE financials SET DBPROPERTIES ('edited-by' = 'active steps');
                </pre>
            </div>

            <p>There is no way to delete or “unset” a DBPROPERTY.</p>
            <p>Create Tables</p>
            <p>The CREATE TABLE statement follows SQL conventions, but Hive’s version offers sig- nificant extensions to support a wide range of flexibility where the data files for tables are stored, the formats used, etc.</p>
            <p>Managed Tables</p>
            <p>The tables we have created so far are called managed tables or sometimes called internal tables, because Hive controls the lifecycle of their data. As we’ve seen,Hive stores the data for these tables in subdirectory under the directory defined by hive.metastore.warehouse.dir (e.g., /user/hive/warehouse), by default.</p>
            <p>When we drop a managed table, Hive deletes the data in the table.</p>
            <p>Managed tables are less convenient for sharing with other tools</p>
            <p>External Tables</p>
            <div class="codebox">
                <pre>
CREATE EXTERNAL TABLE IF NOT EXISTS stocks (
exchange STRING,
symbol STRING,
ymd STRING,
price_open FLOAT,
price_high FLOAT,
price_low FLOAT,
price_close FLOAT,
volume INT,
price_adj_close FLOAT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/data/stocks/';
                </pre>
            </div>

            <p>The EXTERNAL keyword tells Hive this table is external and the LOCATION … clause is required to tell Hive where it’s located. Because it’s external, Hive does not assume it owns the data. Therefore, dropping the table does not delete the data, although the metadata for the table will be deleted.</p>
            <p>Partitioned, Managed Tables</p>
            <p>Partitioned tables help to organize data in a logical fashion, such as hierarchically.</p>
            <p>Example:Our HR people often run queries with WHERE clauses that restrict the results to a particular country or to a particular first-level subdivision (e.g., state in the United States or province in Canada).</p>
            <p>we have to use address.state to project the value inside the address. So, let’s partition the data first by country and then by state:</p>
            <div class="codebox">
                <pre>
CREATE TABLE IF NOT EXISTS mydb.employees (
 name STRING,
 salary FLOAT,
 subordinates ARRAY&lt;STRING&gt;,
 deductions MAP&lt;STRING, FLOAT&gt;,
 address STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;
)
PARTITIONED BY (country STRING, state STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '&#92;&#48;01'
COLLECTION ITEMS TERMINATED BY '&#92;&#48;02'
MAP KEYS TERMINATED BY '&#92;&#48;03'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
                </pre>
            </div>

            <p>Partitioning tables changes how Hive structures the data storage. If we create this table in the mydb database, there will still be an employees directory for the table:</p>
            <div class="codebox">
                <pre>
LOAD DATA LOCAL INPATH '/path/to/employees.txt'
INTO TABLE employees
PARTITION (country = 'US', state = 'IL');
hdfs://master_server/user/hive/warehouse/mydb.db/employees
                </pre>
            </div>

            <p>However, Hive will now create subdirectories reflecting the partitioning structure. For example:</p>
            <div class="codebox">
                <pre>
...
.../employees/country=CA/state=AB
.../employees/country=CA/state=BC
...
.../employees/country=US/state=AL
.../employees/country=US/state=AK
                </pre>
            </div>

            <p>Those are the actual directory names. The state directories will contain zero or more files for the employees in those states.</p>
            <p>Once created, the partition keys (country and state, in this case) behave like regular columns.</p>
            <div class="codebox">
                <pre>
hive (mydb)&gt; SHOW PARTITIONS employees;
OK
country=US/state=IL
Time taken: 0.145 seconds
hive (mydb)&gt; SELECT * FROM employees WHERE country ='US' AND state ='IL';
OK
John Doe 100000.0 [&quot;Mary Smith&quot;,&quot;Todd Jones&quot;] {&quot;Federal Taxes&quot;:0.2,&quot;State Taxes&quot;:0.05,&quot;Insurance&quot;:0.1} {&quot;street&quot;:&quot;1 Michigan Ave.&quot;,&quot;city&quot;:&quot;Chicago&quot;,&quot;state&quot;:&quot;IL&quot;,&quot;zip&quot;:60600} US IL
Mary Smith 80000.0 [&quot;Bill King&quot;] {&quot;Federal Taxes&quot;:0.2,&quot;State Taxes&quot;:0.05,&quot;Insurance&quot;:0.1} {&quot;street&quot;:&quot;100 Ontario St.&quot;,&quot;city&quot;:&quot;Chicago&quot;,&quot;state&quot;:&quot;IL&quot;,&quot;zip&quot;:60601} US IL
Todd Jones 70000.0 [] {&quot;Federal Taxes&quot;:0.15,&quot;State Taxes&quot;:0.03,&quot;Insurance&quot;:0.1} {&quot;street&quot;:&quot;200 Chicago Ave.&quot;,&quot;city&quot;:&quot;Oak Park&quot;,&quot;state&quot;:&quot;IL&quot;,&quot;zip&quot;:60700} US IL
Bill King 60000.0 [] {&quot;Federal Taxes&quot;:0.15,&quot;State Taxes&quot;:0.03,&quot;Insurance&quot;:0.1} {&quot;street&quot;:&quot;300 Obscure Dr.&quot;,&quot;city&quot;:&quot;Obscuria&quot;,&quot;state&quot;:&quot;IL&quot;,&quot;zip&quot;:60100} US IL
Boss Man 200000.0 [&quot;John Doe&quot;,&quot;Fred Finance&quot;] {&quot;Federal Taxes&quot;:0.3,&quot;State Taxes&quot;:0.07,&quot;Insurance&quot;:0.05} {&quot;street&quot;:&quot;1 Pretentious Drive.&quot;,&quot;city&quot;:&quot;Chicago&quot;,&quot;state&quot;:&quot;IL&quot;,&quot;zip&quot;:60500} US IL
Fred Finance 150000.0 [&quot;Stacy Accountant&quot;] {&quot;Federal Taxes&quot;:0.3,&quot;State Taxes&quot;:0.07,&quot;Insurance&quot;:0.05} {&quot;street&quot;:&quot;2 Pretentious Drive.&quot;,&quot;city&quot;:&quot;Chicago&quot;,&quot;state&quot;:&quot;IL&quot;,&quot;zip&quot;:60500} US IL
Stacy Accountant 60000.0 [] {&quot;Federal Taxes&quot;:0.15,&quot;State Taxes&quot;:0.03,&quot;Insurance&quot;:0.1} {&quot;street&quot;:&quot;300 Main St.&quot;,&quot;city&quot;:&quot;Naperville&quot;,&quot;state&quot;:&quot;IL&quot;,&quot;zip&quot;:60563} US IL
Time taken: 0.254 seconds
                </pre>
            </div>

            <p>Partition limits the results to employees in Illinois, it is only necessary to scan the contents of one directory. Partitioning can dramatically improve query performance, but only if the partitioning scheme reflects common range filtering (e.g., by locations, timestamp ranges). When we add predicates to WHERE clauses that filter on partition values, these predicates are called partition filters.</p>
            <p>Strict mode</p>
            <p>A query across all partitions could trigger an enormous MapReduce job if the table data and number of partitions are large. A highly suggested safety measure is putting Hive into strict mode, which prohibits queries of partitioned tables without a WHERE clause that filters on partitions. You can set the mode to nonstrict, as in the following session:</p>
            <div class="codebox">
                <pre>
hive&gt; set hive.mapred.mode=strict;
hive&gt; SELECT e.name, e.salary FROM employees e LIMIT 100;
FAILED: Error in semantic analysis: No partition predicate found for
Alias &quot;e&quot; Table &quot;employees&quot;
hive&gt; set hive.mapred.mode=nonstrict;
hive&gt; SELECT e.name, e.salary FROM employees e LIMIT 100;
                </pre>
            </div>
            <hr />

        </div>

        <div id="hiveQueriesPart1" style="display:block;margin-top:-40px;"><br><br>
            <h2>Hive Queries – Part 1</h2>
            <h3><strong>Prerequisites</strong></h3>
            <ul>
                <li>Hadoop must be installed as per installation instructions <a title="Single Node Installation" href="http://www.datascience-labs.com/hadoop/single-node-installation-2/" target="_blank">Hadoop version 2.0.3 installation</a></li>
                <li>Hive must be installed as per installation instructions <a title="Hive-Installation" href="http://www.datascience-labs.com/hive/run-modes/" target="_blank">Hive version 0.10.0 installation</a></li>
                </ul>

                <ul>
                    <li>Load the dataset into HDFS using this <a title="Dataset Load" href="http://www.datascience-labs.com/hive/dataset/" target="_blank">link</a></li>
                </ul>
                <h2 id="data-set-overview"><span style="line-height: 1.3;">Tables Used In this tutorial</span></h2>
                <ol>
                    <li>Employees</li>
                </ol>
                <h1 id="to-create-the-above-tables-run-the-following-ddl-queries">To create the above tables run the following ddl queries.</h1>
                <p>Create employees</p>
                <div class="codebox">
                    <pre>
CREATE EXTERNAL TABLE employees (
 name STRING,
 salary FLOAT,
 subordinates ARRAY&lt;STRING&gt;,
 deductions MAP&lt;STRING, FLOAT&gt;,
 address STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '&#92;&#48;01'
COLLECTION ITEMS TERMINATED BY '&#92;&#48;02'
MAP KEYS TERMINATED BY '&#92;&#48;03'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION '/data/employees';
                       
                    </pre>
                </div>

                <p><strong>SELECT … FROM Clauses</strong></p>
                <p>SELECT is the projection operator in SQL. The FROM clause identifies from which table, view, or nested query we select records</p>
                <div class="codebox">
                    <pre>
hive&gt; SELECT name, salary FROM employees;
John Doe
100000.0
Mary Smith 80000.0
Todd Jones 70000.0
Bill King 60000.0   
                    </pre>
                </div>

<p>When you select columns that are one of the collection types, Hive uses JSON (Java- Script Object Notation) syntax for the output. First, let’s select the subordinates, an ARRAY, where a comma-separated list surrounded with […] is used. Note that STRING elements of the collection are quoted, while the primitive STRING name column is not</p>
                <div class="codebox">
                    <pre>
hive&gt; SELECT name, subordinates FROM employees;
John Doe
[&quot;Mary Smith&quot;,&quot;Todd Jones&quot;]
Mary Smith [&quot;Bill King&quot;]
Todd Jones []
Bill King [] 
                    </pre>
                </div>

<p>The deductions is a MAP, where the JSON representation for maps is used, namely a comma-separated list of key:value pairs, surrounded with {…}:</p>
                <div class="codebox">
                    <pre>

hive&gt; SELECT name, deductions FROM employees;
John Doe
{&quot;Federal Taxes&quot;:0.2,&quot;State Taxes&quot;:0.05,&quot;Insurance&quot;:0.1}
Mary Smith {&quot;Federal Taxes&quot;:0.2,&quot;State Taxes&quot;:0.05,&quot;Insurance&quot;:0.1}
Todd Jones {&quot;Federal Taxes&quot;:0.15,&quot;State Taxes&quot;:0.03,&quot;Insurance&quot;:0.1}
Bill King {&quot;Federal Taxes&quot;:0.15,&quot;State Taxes&quot;:0.03,&quot;Insurance&quot;:0.1}
Finally, the address is a STRUCT, which is also written using the JSON map format:                        
                    </pre>
                </div>


                <div class="codebox">
                    <pre>

hive&gt; SELECT name, address FROM employees;
John Doe
{&quot;street&quot;:&quot;1 Michigan Ave.&quot;,&quot;city&quot;:&quot;Chicago&quot;,&quot;state&quot;:&quot;IL&quot;,&quot;zip&quot;:60600}
Mary Smith {&quot;street&quot;:&quot;100 Ontario St.&quot;,&quot;city&quot;:&quot;Chicago&quot;,&quot;state&quot;:&quot;IL&quot;,&quot;zip&quot;:60601}
Todd Jones {&quot;street&quot;:&quot;200 Chicago Ave.&quot;,&quot;city&quot;:&quot;Oak Park&quot;,&quot;state&quot;:&quot;IL&quot;,&quot;zip&quot;:60700}
Bill King {&quot;street&quot;:&quot;300 Obscure Dr.&quot;,&quot;city&quot;:&quot;Obscuria&quot;,&quot;state&quot;:&quot;IL&quot;,&quot;zip&quot;:60100}
                        
                    </pre>
                </div>

<p>Next, let’s see how to reference elements of collections. First, ARRAY indexing is 0-based, as in Java. Here is a query that selects the first element of the subordinates array:</p>

                <div class="codebox">
                    <pre>

hive&gt; SELECT name, subordinates[0] FROM employees;
John Doe
Mary Smith
Mary Smith Bill King
Todd Jones NULL
Bill King NULL
                        
                    </pre>
                </div>

<p>Note that referencing a nonexistent element returns NULL. Also, the extracted STRING values are no longer quoted! To reference a MAP element, you also use ARRAY[…] syntax, but with key values instead of integer indices:</p>

                <div class="codebox">
                    <pre>

hive&gt; SELECT name, deductions[&quot;State Taxes&quot;] FROM employees;
John Doe
0.05
Mary Smith 0.05
Todd Jones 0.03
Bill King 0.03                        
                    </pre>
                </div>

<p>Finally, to reference an element in a STRUCT, you use dot notation, similar to the table_alias.column mentioned above:</p>

                <div class="codebox">
                    <pre>

hive&gt; SELECT name, address.city FROM employees;
John Doe
Chicago
Mary Smith Chicago
Todd Jones Oak Park
Bill King Obscuri
LIMIT Clause                        
                    </pre>
                </div>

<p>The results of a typical query can return a large number of rows. The LIMIT clause puts an upper limit on the number of rows returned:</p>

                <div class="codebox">
                    <pre>

hive&gt; SELECT upper(name), salary, deductions[&quot;Federal Taxes&quot;],
&gt; round(salary * (1 - deductions[&quot;Federal Taxes&quot;])) FROM employees
&gt; LIMIT 2;
JOHN DOE 100000.0 0.2 80000
MARY SMITH 80000.0 0.2 64000                        
                    </pre>
                </div>

<p><strong>Column Aliases</strong></p>
<p>You can think of the previous example query as returning a new relation with new columns, some of which are anonymous results of manipulating columns in employees. It’s sometimes useful to give those anonymous columns a name, called a column alias. Here is the previous query with column aliases for the third and fourth columns returned by the query, fed_taxes and salary_minus_fed_taxes, respectively:</p>

                <div class="codebox">
                    <pre>

hive&gt; SELECT upper(name), salary, deductions[&quot;Federal Taxes&quot;] as fed_taxes,
&gt; round(salary * (1 - deductions[&quot;Federal Taxes&quot;])) as salary_minus_fed_taxes
&gt; FROM employees LIMIT 2;

JOHN DOE 100000.0 0.2 80000
MARY SMITH 80000.0 0.2 64000
                        
                    </pre>
                </div>

<p><strong>Nested SELECT Statements</strong></p>
<p>The column alias feature is especially useful in nested select statements. Let’s use the previous example as a nested query:</p>

                <div class="codebox">
                    <pre>

hive&gt; FROM (
&gt; SELECT upper(name), salary, deductions[&quot;Federal Taxes&quot;] as fed_taxes,
&gt; round(salary * (1 - deductions[&quot;Federal Taxes&quot;])) as salary_minus_fed_taxes
&gt; FROM employees
&gt; ) e
&gt; SELECT e.name, e.salary_minus_fed_taxes
&gt; WHERE e.salary_minus_fed_taxes &gt; 70000;
JOHN DOE 100000.0 0.2 80000
                        
                    </pre>
                </div>

<p>The previous result set is aliased as e, from which we perform a second query to select the name and the salary_minus_fed_taxes, where the latter is greater than 70,000.</p>
<p><strong>CASE … WHEN … THEN Statements</strong></p>
<p>The CASE … WHEN … THEN clauses are like if statements for individual columns in query results. For example:</p>

                <div class="codebox">
                    <pre>

hive&gt; SELECT name, salary,
&gt; CASE
&gt;
WHEN salary &lt; 50000.0 THEN 'low'
&gt;
WHEN salary &gt;= 50000.0 AND salary &lt; 70000.0 THEN 'middle'
&gt;
WHEN salary &gt;= 70000.0 AND salary &lt; 100000.0 THEN 'high'
&gt;
ELSE 'very high'
&gt;
END AS bracket FROM employees;
John Doe 100000.0 very high
Mary Smith 80000.0 high
Todd Jones 70000.0 high
Bill King 60000.0 middle
Boss Man 200000.0 very high
Fred Finance 150000.0 very high
Stacy Accountant 60000.0 middle
...                        
                    </pre>
                </div>

<p><strong>WHERE Clauses</strong></p>
<p><strong></strong>WHERE clauses are filters; they select which records to return.WHERE clauses use predicate expressions, applying predicate operators</p>
<p>The predicates can reference the same variety of computations over column values that can be used in SELECT clauses. Here we adapt our previously used query involving Federal Taxes, filtering for those rows where the salary minus the federal taxes is greater than 70,000:</p>

                <div class="codebox">
                    <pre>

hive&gt; SELECT name, salary, deductions[&quot;Federal Taxes&quot;],
&gt; salary * (1 - deductions[&quot;Federal Taxes&quot;])
&gt; FROM employees
&gt; WHERE round(salary * (1 - deductions[&quot;Federal Taxes&quot;])) &gt; 70000;
John Doe 100000.0 0.2 80000.0
                        
                    </pre>
                </div>

<p>This query is a bit ugly, because the complex expression on the second line is duplicated in the WHERE clause. The following variation eliminates the duplication, using a column alias, but unfortunately it’s not valid:</p>

                <div class="codebox">
                    <pre>

hive&gt; SELECT name, salary, deductions[&quot;Federal Taxes&quot;],
&gt;
salary * (1 - deductions[&quot;Federal Taxes&quot;]) as salary_minus_fed_taxes
&gt; FROM employees
&gt; WHERE round(salary_minus_fed_taxes) &gt; 70000;
FAILED: Error in semantic analysis: Line 4:13 Invalid table alias or
column reference 'salary_minus_fed_taxes': (possible column names are:
name, salary, subordinates, deductions, address)                        
                    </pre>
                </div>

<p>As the error message says, we can’t reference column aliases in the WHERE clause. How- ever, we can use a nested SELECT statement:</p>

                <div class="codebox">
                    <pre>

hive&gt; SELECT e.* FROM
&gt; (SELECT name, salary, deductions[&quot;Federal Taxes&quot;] as ded,
&gt;
salary * (1 - deductions[&quot;Federal Taxes&quot;]) as salary_minus_fed_taxes
&gt; FROM employees) e
&gt; WHERE round(e.salary_minus_fed_taxes) &gt; 70000;

John Doe 100000.0 0.2 80000.0
Boss Man 200000.0 0.3 140000.0
Fred Finance 150000.0 0.3 105000.0
                        
                    </pre>
                </div>

<p><strong>LIKE and RLIKE</strong></p>
<p>You have probably seen LIKE before, a standard SQL operator. For example, the following three queries select the employee names and addresses where the street ends with Ave., the city begins with O, and the street contains Chicago:</p>

                <div class="codebox">
                    <pre>

hive&gt; SELECT name, address.street FROM employees WHERE address.street LIKE '%Ave.';
John Doe 1 Michigan Ave.
Todd Jones 200 Chicago Ave.
                        
                    </pre>
                </div>


                <div class="codebox">
                    <pre>
hive&gt; SELECT name, address.city FROM employees WHERE address.city LIKE 'O%';
Todd Jones Oak Park
Bill King Obscuria                        
                    </pre>
                </div>



                <div class="codebox">
                    <pre>
hive&gt; SELECT name, address.street FROM employees WHERE address.street LIKE '%Chi%';
Todd Jones 200 Chicago Ave.
                        
                    </pre>
                </div>

<p>A Hive extension is the RLIKE clause, which lets us use Java regular expressions, a more powerful minilanguage for specifying matches.</p>

                <div class="codebox">
                    <pre>

hive&gt; SELECT name, address.street
&gt; FROM employees WHERE address.street RLIKE '.*(Chicago|Ontario).*';

Mary Smith 100 Ontario St.
Todd Jones 200 Chicago Ave.
                        
                    </pre>
                </div>

<p>The string after the RLIKE keyword has the following interpretation. A period . matches any character and a star * means repeat the “thing to the left” (period, in the two cases shown) zero to many times. The expression x|y means match either x or y. Hence, there might be no characters before “Chicago” or “Ontario” and there might be no characters after them. Of course, we could have written this particular example with two LIKE clauses:</p>

                <div class="codebox">
                    <pre>
SELECT name, address FROM employees
WHERE address.street LIKE '%Chicago%' OR address.street LIKE '%Ontario%';
                        
                    </pre>
                </div>

<p>General regular expression matches will let us express much richer matching criteria that would become very unwieldy with joined LIKE clauses such as these.</p>
    
        </div>

        <div id="hiveQueriesPart2" style="display:block;margin-top:-40px;"><br><br>
            <h2>Hive Queries – Part 2</h2>
            <p>Hive supports the classic SQL JOIN statement, but only equi-joins are supported.</p>
            <h2 id="prerequisites">Prerequisites</h2>
            <ul>
                <li>Hadoop must be installed as per installation instructions <a>Hadoop version 2.0.3 installation</a></li>
                <li>Hive must be installed as per installation instructions <a>Hive version 0.10.1 installation</a></li>
            </ul>

            <ul>
                <li>Load the dataset into HDFS using this <a title="Dataset Load" href="http://www.datascience-labs.com/hive/dataset/" target="_blank">link</a></li>
            </ul>
            <h2>Data set overview</h2>
            <p>Historical NYSE stock data from 1970 – 2010, including daily open, close, low, high and trading volume figures. Data is organized alphabetically by ticker symbol.</p>
            <h2>Tables Used In this tutorial</h2>
            <ol>
                <li>Dividends</li>
                <li>Stocks</li>
                <li>Employees</li>
            </ol>
            <h3>To create the above tables run the following ddl queries.</h3>
            <p>Create stocks</p>
            <div class="codebox">
                <pre>

CREATE EXTERNAL TABLE IF NOT EXISTS stocks (
exchange STRING,
symbol STRING,
ymd STRING,
price_open FLOAT,
price_high FLOAT,
price_low FLOAT,
price_close FLOAT,
volume INT,
price_adj_close FLOAT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/data/stocks/';
Queries on Sotck Data Set

                </pre>
            </div>

            <p><strong>Load the stocks</strong></p>
            <div class="codebox">
                <pre>

LOAD DATA LOCAL INPATH '/Users/reddyraja/soft/Hive-Demo/data/stocks/input/plain-text/NASDAQ/AAPL/stocks.csv'
INTO TABLE stocks
PARTITION (exchange = 'NASDAQ', symbol = 'AAPL');

                </pre>
            </div>

            <p>Load the stocks</p>
            <div class="codebox">
                <pre>

LOAD DATA LOCAL INPATH '/Users/reddyraja/soft/Hive-Demo/data/stocks/input/plain-text/NASDAQ/INTC/stocks.csv'
INTO TABLE stocks
PARTITION (exchange = 'NASDAQ', symbol = 'INTC');

                </pre>
            </div>

<p><strong>Load the stocks</strong></p>
            <div class="codebox">
                <pre>

LOAD DATA LOCAL INPATH '/Users/reddyraja/soft/Hive-Demo/data/stocks/input/plain-text/NYSE/GE/stocks.csv'
INTO TABLE stocks
PARTITION (exchange = 'NYSE', symbol = 'GE');

                </pre>
            </div>

            <p><strong>Load the stocks</strong></p>
            <div class="codebox">
                <pre>

LOAD DATA LOCAL INPATH '/Users/reddyraja/soft/Hive-Demo/data/stocks/input/plain-text/NYSE/IBM/stocks.csv'
INTO TABLE stocks
PARTITION (exchange = 'NYSE', symbol = 'IBM');

                </pre>
            </div>

            <p>Queries on Stocks Table</p>
            <div class="codebox">
                <pre>

SELECT * FROM stocks WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' LIMIT 10;
SELECT ymd, price_close FROM stocks WHERE exchange = 'NASDAQ' AND symbol = 'AAPL' LIMIT 10;
SELECT ymd, price_open, price_close
FROM stocks WHERE symbol = 'AAPL' AND exchange = 'NASDAQ' LIMIT 20;
SELECT count(*) FROM stocks
WHERE symbol = 'AAPL' AND exchange = 'NASDAQ';
Average the closing price for AAPL: ($51.75)

                </pre>
            </div>

            <div class="codebox">
                <pre>

SELECT avg(price_close) FROM stocks
 WHERE symbol = 'AAPL' AND exchange = 'NASDAQ';
 SELECT year(ymd), avg(price_close)
 FROM stocks
 WHERE symbol = 'AAPL' AND exchange = 'NASDAQ'
 GROUP BY year(ymd);
                </pre>
            </div>

            <p>GROUP BY &#8230; HAVING is a way of further filtering the output.</p>
            <div class="codebox">
                <pre>

SELECT year(ymd), avg(price_close)
 FROM stocks
 WHERE symbol = 'AAPL' AND exchange = 'NASDAQ'
 GROUP BY year(ymd)
 HAVING avg(price_close) &gt; 50.0 AND avg(price_close) &lt; 100.0;

                </pre>
            </div>

            <p><strong>Dividends</strong></p>
            <p><strong>Create dividends</strong></p>
            <div class="codebox">
                <pre>

CREATE TABLE IF NOT EXISTS dividends (
 ymd STRING,
 dividend FLOAT
 )
 PARTITIONED BY (exchange STRING, symbol STRING)
 ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
 LOCATION '/data/dividends';

                </pre>
            </div>

            <p><strong>Load Dividend Data</strong></p>
            <div class="codebox">
                <pre>

LOAD DATA LOCAL INPATH '/path/to/data/dividends/input/plain-text/NASDAQ/AAPL/dividends.csv'
 INTO TABLE DIVIDENDS
 PARTITION (exchange = 'NASDAQ', symbol = 'AAPL');
 LOAD DATA LOCAL INPATH '/path/to/data/dividends/input/plain-text/NASDAQ/INTC/dividends.csv'
 INTO TABLE DIVIDENDS
 PARTITION (exchange = 'NASDAQ', symbol = 'INTC');
 LOAD DATA LOCAL INPATH '/path/to/data/dividends/input/plain-text/NYSE/GE/dividends.csv'
 INTO TABLE DIVIDENDS
 PARTITION (exchange = 'NYSE', symbol = 'GE');
 LOAD DATA LOCAL INPATH '/path/to/data/dividends/input/plain-text/NYSE/IBM/dividends.csv'
 INTO TABLE DIVIDENDS
 PARTITION (exchange = 'NYSE', symbol = 'IBM');

                </pre>
            </div>
            <p><strong>Queries on Dividends</strong></p>
            <p>display 10 records of dividends table</p>
            <div class="codebox">
                <pre>

hive&gt; SELECT * FROM dividends LIMIT 10;
 display 10 records of stocks table

hive&gt; SELECT * FROM stocks LIMIT 10;
 Inner JOIN

                </pre>
            </div>

            <p>In an inner JOIN, records are discarded unless join criteria finds matching records in every table being joined. For example, the following query compares Apple (symbol AAPL) and IBM (symbol IBM). The stocks table is joined against itself, a self-join, where the dates, ymd (year-month-day) values must be equal in both tables. We say that the ymd columns are the join keys in this query:</p>
            <div class="codebox">
                <pre>

hive&gt; SELECT a.ymd, a.price_close, b.price_close
 &gt; FROM stocks a JOIN stocks b ON a.ymd = b.ymd
 &gt; WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';
 2010-01-04 214.01 132.45
 2010-01-05 214.38 130.85
 2010-01-06 210.97 130.0
 2010-01-07 210.58 129.55
 2010-01-08 211.98 130.85
 2010-01-11 210.11 129.48
 ...

                </pre>
            </div>

            <p>The ON clause specifies the conditions for joining records between the two tables. The WHERE clause limits the lefthand table to AAPL records and the righthand table to IBM records. You can also see that using table aliases for the two occurrences of stocks is essential in this query.</p>
            <p>As you may know, IBM is an older company than Apple. It has been a publicly traded stock for much longer than Apple. However, since this is an inner JOIN, no IBM records will be returned older than September 7, 1984, which was the first day that Apple was publicly traded!</p>
            <p>Standard SQL allows a non-equi-join on the join keys, such as the following example that shows Apple versus IBM, but with all older records for Apple paired up with each day of IBM data. It would be a lot of data</p>
            <div class="codebox">
                <pre>

SELECT a.ymd, a.price_close, b.price_close
 FROM stocks a JOIN stocks b
 ON a.ymd &lt;= b.ymd
 WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM';

                </pre>
            </div>

            <p>This is not valid in Hive, primarily because it is difficult to implement these kinds of joins in MapReduce. Also, Hive does not currently support using OR between predicates in ON clauses.</p>
            <p>Here is an inner JOIN between stocks and dividends for Apple, where we use the ymd and symbol columns as join keys:</p>
            <div class="codebox">
                <pre>

hive&gt; SELECT s.ymd, s.symbol, s.price_close, d.dividend
 &gt; FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol
 &gt; WHERE s.symbol = 'AAPL';
 1987-05-11 AAPL 77.0 0.015
 1987-08-10 AAPL 48.25 0.015
 1987-11-17 AAPL 35.0 0.02
 ...
 1995-02-13 AAPL 43.75 0.03
 1995-05-26 AAPL 42.69 0.03
 1995-08-16 AAPL 44.5 0.03
 1995-11-21 AAPL 38.63 0.03

                </pre>
            </div>

            <p>You can join more than two tables together. Let’s compare Apple, IBM, and GE side by side:</p>
            <div class="codebox">
                <pre>

hive&gt; SELECT a.ymd, a.price_close, b.price_close , c.price_close
 &gt; FROM stocks a JOIN stocks b ON a.ymd = b.ymd
 &gt;
 JOIN stocks c ON a.ymd = c.ymd
 &gt; WHERE a.symbol = 'AAPL' AND b.symbol = 'IBM' AND c.symbol = 'GE';
 2010-01-04
 214.01 132.45 15.45
 2010-01-05
 214.38 130.85 15.53
 2010-01-06
 210.97 130.0
 15.45
 2010-01-07
 210.58 129.55 16.25
 2010-01-08
 211.98 130.85 16.6
 2010-01-11
 210.11 129.48 16.76
 ...

                </pre>
            </div>

            <p>Most of the time, Hive will use a separate MapReduce job for each pair of things to join. In this example, it would use one job for tables a and b, then a second job to join the output of the first join with c.</p>
            <p><strong>Join Optimizations</strong></p>
            <p>When joining three or more tables, if every ON clause uses the same join key, a single MapReduce job will be used.</p>
            <p>Hive also assumes that the last table in the query is the largest. It attempts to buffer the other tables and then stream the last table through, while performing joins on individual records. Therefore, you should structure your join queries so the largest table is last. Recall our previous join between stocks and dividends. We actually made the mistake of using the smaller dividends table last:</p>
            <div class="codebox">
                <pre>

SELECT s.ymd, s.symbol, s.price_close, d.dividend
 FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol
 WHERE s.symbol = 'AAPL';
 We should switch the positions of stocks and dividends:

SELECT s.ymd, s.symbol, s.price_close, d.dividend
 FROM dividends d JOIN stocks s ON s.ymd = d.ymd AND s.symbol = d.symbol
 WHERE s.symbol = 'AAPL';

                </pre>
            </div>

            <p>Fortunately, you don’t have to put the largest table last in the query. Hive also provides a “hint” mechanism to tell the query optimizer which table should be streamed:</p>
            <div class="codebox">
                <pre>

SELECT /*+ STREAMTABLE(s) */ s.ymd, s.symbol, s.price_close, d.dividend
 FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol
 WHERE s.symbol = 'AAPL';

                </pre>
            </div>

            <p>Now Hive will attempt to stream the stocks table, even though it’s not the last table in the query.</p>
            <p><strong>LEFT OUTER JOIN</strong></p>
            <p>The left-outer join is indicated by adding the LEFT OUTER keywords:</p>
            <div class="codebox">
                <pre>

hive&gt; SELECT s.ymd, s.symbol, s.price_close, d.dividend
 &gt; FROM stocks s LEFT OUTER JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol
 &gt; WHERE s.symbol = 'AAPL';
 ...
 1987-05-01 AAPL 80.0 NULL
 1987-05-04 AAPL 79.75 NULL
 1987-05-05 AAPL 80.25 NULL
 1987-05-06 AAPL 80.0 NULL
 1987-05-07 AAPL 80.25 NULL
 1987-05-08 AAPL 79.0 NULL
 1987-05-11 AAPL 77.0 0.015
 1987-05-12 AAPL 75.5 NULL
 1987-05-13 AAPL 78.5 NULL
 1987-05-14 AAPL 79.25 NULL
 1987-05-15 AAPL 78.25 NULL
 1987-05-18 AAPL 75.75 NULL
 1987-05-19 AAPL 73.25 NULL
 1987-05-20 AAPL 74.5 NULL
 ...

                </pre>
            </div>

            <p>In this join, all the records from the lefthand table that match the WHERE clause are returned. If the righthand table doesn’t have a record that matches the ON criteria, NULL is used for each column selected from the righthand table. Hence, in this result set, we see that the every Apple stock record is returned and the d.dividend value is usually NULL, except on days when a dividend was paid (May 11th, 1987, in this output).</p>
            <p><strong>OUTER JOIN Gotcha</strong></p>
            <p><strong>RIGHT OUTER JOIN</strong></p>
            <p>Right-outer joins return all records in the righthand table that match the WHERE clause. NULL is used for fields of missing records in the lefthand table. Here we switch the places of stocks and dividends and perform a righthand join, but leave the SELECT statement unchanged:</p>
            <div class="codebox">
                <pre>

hive&gt; SELECT s.ymd, s.symbol, s.price_close, d.dividend
 &gt; FROM dividends d RIGHT OUTER JOIN stocks s ON d.ymd = s.ymd AND d.symbol = s.symbol
 &gt; WHERE s.symbol = 'AAPL';
 ...
 1987-05-07 AAPL 80.25 NULL
 1987-05-08 AAPL 79.0 NULL
 1987-05-11 AAPL 77.0 0.015
 1987-05-12 AAPL 75.5 NULL
 1987-05-13 AAPL 78.5 NULL
 ...

                </pre>
            </div>

            <p><strong>LEFT SEMI-JOIN</strong></p>
            <p>A left semi-join returns records from the lefthand table if records are found in the right- hand table that satisfy the ON predicates. It’s a special, optimized case of the more general inner join. Most SQL dialects support an IN … EXISTS construct to do the same thing. For instance, the following query in Example 6-2 attempts to return stock records only on the days of dividend payments, but it doesn’t work in Hive. Example 6-2. Query that will not work in Hive SELECT s.ymd, s.symbol, s.price_close FROM stocks s WHERE s.ymd, s.symbol IN (SELECT d.ymd, d.symbol FROM dividends d); Instead, you use the following LEFT SEMI JOIN syntax:</p>
            <div class="codebox">
                <pre>

hive&gt; SELECT s.ymd, s.symbol, s.price_close
 &gt; FROM stocks s LEFT SEMI JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol;
 ...
 1962-11-05 IBM 361.5
 1962-08-07 IBM 373.25
 1962-05-08 IBM 459.5
 1962-02-06 IBM 551.5

                </pre>
            </div>

            <p>Note that the SELECT and WHERE clauses can’t reference columns from the righthand table.</p>
            <p>The reason semi-joins are more efficient than the more general inner join is as follows. For a given record in the lefthand table, Hive can stop looking for matching records in the righthand table as soon as any match is found. At that point, the selected columns from the lefthand table record can be projected.</p>
            <p><strong>Cartesian Product JOINs</strong></p>
            <p>A Cartesian product is a join where all the tuples in the left side of the join are paired with all the tuples of the right table. If the left table has 5 rows and the right table has 6 rows, 30 rows of output will be produced:</p>
            <div class="codebox">
                <pre>

SELECT * FROM stocks JOIN dividends;

                </pre>
            </div>

            <p>Cartesian products create a lot of data. Unlike other join types, Cartesian products are not executed in parallel, and they are not optimized in any way using MapReduce.</p>
            <p><strong>Map-side Joins</strong></p>
            <p>If all but one table is small, the largest table can be streamed through the mappers while the small tables are cached in memory. Hive can do all the joining map-side, since it can look up every possible match against the small tables in memory, thereby eliminating the reduce step required in the more common join scenarios.</p>
            <p>The joins between stocks and dividends can exploit this optimization, as the dividends data set is small enough to be cached.</p>
            <p>set hive.auto.convert.join, to true before Hive will attempt the optimization. It’s false by default:</p>
            <div class="codebox">
                <pre>

hive&gt; set hive.auto.convert.join=true;
 hive&gt; SELECT s.ymd, s.symbol, s.price_close, d.dividend
 &gt; FROM stocks s JOIN dividends d ON s.ymd = d.ymd AND s.symbol = d.symbol
 &gt; WHERE s.symbol = 'AAPL';

                </pre>
            </div>

            <p>Note that you can also configure the threshold size for table files considered small enough to use this optimization.</p>
            <p>Hive does not support the optimization for right- and full-outer joins.</p>
            <p>Map-side joins can be done on big tables when data for every table is bucketed.</p>
            <p>Map-side join can be enabled by setting the property</p>
            <div class="codebox">
                <pre>

hive.optimize.bucketmapjoin: set hive.optimize.bucketmapjoin=true;

                </pre>
            </div>

            <p>If the bucketed tables actually have the same number of buckets and the data is sorted by the join/bucket keys, then Hive can perform an even faster sort-merge join. Once again, properties must be set to enable the optimization:</p>
            <p>set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat set hive.optimize.bucketmapjoin=true set hive.optimize.bucketmapjoin.sortedmerge=true</p>
            <p><strong>ORDER BY and SORT BY</strong></p>
            <p>ORDER BY performs a total ordering of the query result set. This means that all the data is passed through a single reducer, which may take an unacceptably long time to execute for larger data sets.</p>
            <p>Hive adds an alternative, SORT BY, that orders the data only within each reducer, thereby performing a local ordering, where each reducer’s output will be sorted. Better perfor- mance is traded for total ordering. In both cases, the syntax differs only by the use of the ORDER or SORT keyword. You can specify any columns you wish and specify whether or not the columns are ascending using the ASC keyword (the default) or descending using the DESC keyword. Here is an example using ORDER BY:</p>
            <div class="codebox">
                <pre>

SELECT s.ymd, s.symbol, s.price_close
 FROM stocks s
 ORDER BY s.ymd ASC, s.symbol DESC;

                </pre>
            </div>

            <p>Here is the same example using <strong>SORT BY</strong> instead:</p>
            <div class="codebox">
                <pre>

SELECT s.ymd, s.symbol, s.price_close
 FROM stocks s
 SORT BY s.ymd ASC, s.symbol DESC;

                </pre>
            </div>

            <p>The two queries look almost identical, but if more than one reducer is invoked, the output will be sorted differently. While each reducer’s output files will be sorted, the data will probably overlap with the output of other reducers. Because<strong> ORDER BY</strong> can result in excessively long run times, Hive will require a <strong>LIMIT</strong> clause with <strong>ORDER BY</strong> if the property <strong>hive.mapred.mode</strong> is set to strict. By default, it is set to nonstrict.</p>
            <p><strong>DISTRIBUTE BY with SORT BY</strong></p>
            <p><strong>DISTRIBUTE BY</strong> controls how map output is divided among reducers. All data that flows through a MapReduce job is organized into key-value pairs.MapReduce computes a hash on the keys output by mappers and tries to evenly distribute the key-value pairs among the available reducers using the hash values. this means that when we use SORT BY, the contents of one reducer’s output will overlap significantly with the output of the other reducers, as far as sorted order is concerned, even though the data is sorted within each reducer’s output.</p>
            <p>Say we want the data for each stock symbol to be captured together. We can use<strong> DISTRIBUTE BY</strong> to ensure that the records for each stock symbol go to the same reducer, then use <strong>SORT BY</strong> to order the data the way we want. The following query demonstrates this technique:</p>
            <div class="codebox">
                <pre>

hive&gt; SELECT s.ymd, s.symbol, s.price_close
 &gt; FROM stocks s
 &gt; DISTRIBUTE BY s.symbol
 &gt; SORT BY s.symbol ASC, s.ymd ASC;
 1984-09-07 AAPL 26.5
 1984-09-10 AAPL 26.37
 1984-09-11 AAPL 26.87
 1984-09-12 AAPL 26.12
 1984-09-13 AAPL 27.5
 1984-09-14 AAPL 27.87
 1984-09-17 AAPL 28.62
 1984-09-18 AAPL 27.62
 1984-09-19 AAPL 27.0
 1984-09-20 AAPL 27.12
 ...

                </pre>
            </div>

            <p><strong>DISTRIBUTE BY</strong> works similar to <strong>GROUP BY</strong> in the sense that it controls how reducers receive rows for processing, while <strong>SORT BY</strong> controls the sorting of data inside the reducer.</p>

        </div><!--/hiveQueriesPart2-->


            </div>
        </div>


<div class="col-md-3" id="rightbar">
    <div class="panel panel-default" style="height:1000px;">

                <div class="panel">
                    Ads 
                </div>


    </div>

</div>

</div><!--row-->

  </div><!--container-->

  <script type="text/javascript">
    $('a[href^="#"]').on('click', function(event) {

    var target = $( $(this).attr('href') );

    if( target.length ) {
        event.preventDefault();
        $('html, body').animate({
            scrollTop: target.offset().top
        }, 1000);
    }

});
</script>


